{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hello world Transformers ðŸ¤—\n",
    "\n",
    "In this notebook we will explore the basics of the Hugging Face library by using pre-trained models to classify text.\n",
    "\n",
    "âš ï¸ Do not forget to install the `transformers` library to run this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: uv in c:\\users\\cestm\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.9.11)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2mUsing Python 3.12.6 environment at: C:\\Users\\cestm\\AppData\\Local\\Programs\\Python\\Python312\u001b[0m\n",
      "\u001b[2mResolved \u001b[1m34 packages\u001b[0m \u001b[2min 831ms\u001b[0m\u001b[0m\n",
      "\u001b[36m\u001b[1mDownloading\u001b[0m\u001b[39m transformers \u001b[2m(11.4MiB)\u001b[0m\n",
      "\u001b[36m\u001b[1mDownloading\u001b[0m\u001b[39m sentencepiece \u001b[2m(1.0MiB)\u001b[0m\n",
      "\u001b[36m\u001b[1mDownloading\u001b[0m\u001b[39m tokenizers \u001b[2m(2.6MiB)\u001b[0m\n",
      " Downloaded sentencepiece\n",
      " Downloaded tokenizers\n",
      " Downloaded transformers\n",
      "\u001b[2mPrepared \u001b[1m6 packages\u001b[0m \u001b[2min 5.95s\u001b[0m\u001b[0m\n",
      "\u001b[2mInstalled \u001b[1m6 packages\u001b[0m \u001b[2min 4.68s\u001b[0m\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mhuggingface-hub\u001b[0m\u001b[2m==0.36.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1msacremoses\u001b[0m\u001b[2m==0.1.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1msafetensors\u001b[0m\u001b[2m==0.7.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1msentencepiece\u001b[0m\u001b[2m==0.2.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtokenizers\u001b[0m\u001b[2m==0.22.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtransformers\u001b[0m\u001b[2m==4.57.3\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Install dependencies using uv\n",
    "import sys\n",
    "!{sys.executable} -m pip install uv\n",
    "!uv pip install transformers torch pandas sentencepiece sacremoses --system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "from transformers import pipeline, set_seed\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick overview of Transformer applications\n",
    "\n",
    "Let's start by defining a text that we will use to test the model.\n",
    "\n",
    "> For testing purposes, we will use a text that is a complaint about a product. You can generate your own text or change the text to test the model with different inputs ðŸ˜Š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dear Amazon, last week I ordered an Optimus Prime action figure from your online store in Germany. Unfortunately, when I opened the package, I discovered to my horror that I had been sent an action figure of Megatron instead! As a lifelong enemy of the Decepticons, I hope you can understand my dilemma. To resolve the issue, I demand an exchange of Megatron for the Optimus Prime figure I ordered. Enclosed are copies of my records concerning this purchase. I expect to hear from you soon. Sincerely, Bumblebee.\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"Dear Amazon, last week I ordered an Optimus Prime action figure from your online store in Germany. Unfortunately, when I opened the package, I discovered to my horror that I had been sent an action figure of Megatron instead! As a lifelong enemy of the Decepticons, I hope you can understand my dilemma. To resolve the issue, I demand an exchange of Megatron for the Optimus Prime figure I ordered. Enclosed are copies of my records concerning this purchase. I expect to hear from you soon. Sincerely, Bumblebee.\"\"\"\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Text Classification\n",
    "\n",
    "### ðŸ“š Question 1: Understanding Pipelines\n",
    "\n",
    "Before we start using the models, let's understand what we're working with:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. What is a `pipeline` in Hugging Face Transformers? What does it abstract away from the user?\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "A `pipeline` is a high-level API in Hugging Face Transformers that abstracts away several complex steps:\n",
    "\n",
    "- **Preprocessing**: Tokenization of input text (converting text to tokens/IDs that the model understands)\n",
    "- **Model inference**: Running the preprocessed input through the neural network\n",
    "- **Postprocessing**: Converting model outputs (raw logits) into human-readable results (labels, scores, etc.)\n",
    "\n",
    "The pipeline handles all the technical details so users can focus on the task without worrying about tokenization, tensor manipulation, or decoding outputs.\n",
    "\n",
    "#### 2. Visit the [pipeline documentation](https://huggingface.co/docs/transformers/main/en/pipeline_tutorial) and list at least 3 other tasks (besides text-classification) that are available.\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "Other available tasks include:\n",
    "- **question-answering**: Extract answers from context\n",
    "- **text-generation**: Generate text continuations\n",
    "- **summarization**: Summarize long documents\n",
    "- **translation**: Translate between languages\n",
    "- **named-entity-recognition (NER)**: Identify entities in text\n",
    "- **sentiment-analysis**: Same as text-classification\n",
    "- **zero-shot-classification**: Classify without training examples\n",
    "\n",
    "#### 3. What happens when you don't specify a model in the pipeline? How can you specify a specific model?\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "When you don't specify a model, the pipeline automatically loads a **default pre-trained model** for that task. For example, for text-classification, it loads `distilbert-base-uncased-finetuned-sst-2-english`.\n",
    "\n",
    "To specify a specific model:\n",
    "```python\n",
    "classifier = pipeline(\"text-classification\", model=\"your-model-name\")\n",
    "# Example:\n",
    "classifier = pipeline(\"text-classification\", model=\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ’¡ **Hint:** Check the official documentation to answer these questions!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First thing we will do is to classify the text into two categories: positive or negative.\n",
    "\n",
    "To do this, we will use a pre-trained model from the Hugging Face library.\n",
    "\n",
    "We will use the `pipeline` function to load the model and the `text-classification` task.\n",
    "\n",
    "See the documentation for more details:\n",
    "https://huggingface.co/docs/transformers/main/en/pipeline_tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eddeec55ee114c46878efbfab684955b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/629 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\cestm\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d62c8c40e8ad45cc89477af01bbda458",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6182cd676105479bbf7a5e9feb4b32b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45982385e56d451dbd8b639456c1a173",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NEGATIVE</td>\n",
       "      <td>0.901546</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      label     score\n",
       "0  NEGATIVE  0.901546"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create text classification pipeline\n",
    "classifier = pipeline(\"text-classification\")\n",
    "\n",
    "# Classify the text\n",
    "outputs = classifier(text)\n",
    "pd.DataFrame(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### ðŸ“š Question 2: Text Classification Deep Dive\n",
    "\n",
    "Now that you've seen text classification in action, explore further:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. What is the default model used for text-classification? Look at the output above to find its name, then search for it on the [Hugging Face Model Hub](https://huggingface.co/models).\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "The default model is: **`distilbert-base-uncased-finetuned-sst-2-english`**\n",
    "\n",
    "You can find it at: https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english\n",
    "\n",
    "#### 2. What dataset was this model fine-tuned on? What kind of text does it work best with?\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "- **Dataset**: SST-2 (Stanford Sentiment Treebank)\n",
    "- **Text type**: Movie reviews and general sentiment analysis\n",
    "- **Best for**: Short English texts expressing opinions (reviews, comments, feedback)\n",
    "\n",
    "#### 3. The output includes a `score` field. What does this score represent? What range of values can it have?\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "- **Represents**: The model's confidence in its prediction (probability)\n",
    "- **Range**: 0 to 1 (0% to 100% confidence)\n",
    "- Higher scores mean the model is more confident in its classification\n",
    "\n",
    "#### 4. **Challenge**: Find a different text-classification model on the Hub that classifies emotions (not just positive/negative). What is its name?\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "Example models for emotion classification:\n",
    "- **`j-hartmann/emotion-english-distilroberta-base`** - Classifies 7 emotions (joy, sadness, anger, fear, surprise, disgust, neutral)\n",
    "- **`bhadresh-savani/distilbert-base-uncased-emotion`** - Classifies 6 emotions\n",
    "- **`SamLowe/roberta-base-go_emotions`** - Classifies 28 emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff3322568d504549b846019623fdd2ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a9b5c98676545c3bf7cc41807a1d9e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/329M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e90d27b6ec14afdb413319edc796fa3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/329M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08b183a7b0bf42739875fe10d24c801f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/294 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "560e52aab6f548749fdd58edb7d114e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01d8eef0c8a548de9a011e612abba8e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "181167e4a8494fe6affc65e0ae0d794e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b38ed1b771da408d96437613b5633f51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Emotion Classification:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fear</td>\n",
       "      <td>0.706279</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label     score\n",
       "0  fear  0.706279"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's test an emotion classification model\n",
    "emotion_classifier = pipeline(\"text-classification\", model=\"j-hartmann/emotion-english-distilroberta-base\")\n",
    "emotion_outputs = emotion_classifier(text)\n",
    "print(\"\\nEmotion Classification:\")\n",
    "pd.DataFrame(emotion_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ’¡ **Hint:** Click on the model card in the Hub to see detailed information about training data and performance!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Named Entity Recognition\n",
    "\n",
    "Let's try Named Entity Recognition (NER) to extract entities from our text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision 4c53496 (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4788bf2cc6c4c3cb6086534ba40a361",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/998 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7395e6a9fad04bc2bd20f893657a16ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.33G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c72bb16c645344c2b4834daa1a77f631",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/60.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c9411bb85254661addb8dda35064344",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>entity_group</th>\n",
       "      <th>score</th>\n",
       "      <th>word</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ORG</td>\n",
       "      <td>0.879011</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MISC</td>\n",
       "      <td>0.990859</td>\n",
       "      <td>Optimus Prime</td>\n",
       "      <td>36</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LOC</td>\n",
       "      <td>0.999755</td>\n",
       "      <td>Germany</td>\n",
       "      <td>90</td>\n",
       "      <td>97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MISC</td>\n",
       "      <td>0.556570</td>\n",
       "      <td>Mega</td>\n",
       "      <td>208</td>\n",
       "      <td>212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PER</td>\n",
       "      <td>0.590255</td>\n",
       "      <td>##tron</td>\n",
       "      <td>212</td>\n",
       "      <td>216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ORG</td>\n",
       "      <td>0.669692</td>\n",
       "      <td>Decept</td>\n",
       "      <td>253</td>\n",
       "      <td>259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>MISC</td>\n",
       "      <td>0.498350</td>\n",
       "      <td>##icons</td>\n",
       "      <td>259</td>\n",
       "      <td>264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>MISC</td>\n",
       "      <td>0.775362</td>\n",
       "      <td>Megatron</td>\n",
       "      <td>350</td>\n",
       "      <td>358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>MISC</td>\n",
       "      <td>0.987854</td>\n",
       "      <td>Optimus Prime</td>\n",
       "      <td>367</td>\n",
       "      <td>380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>PER</td>\n",
       "      <td>0.812096</td>\n",
       "      <td>Bumblebee</td>\n",
       "      <td>502</td>\n",
       "      <td>511</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  entity_group     score           word  start  end\n",
       "0          ORG  0.879011         Amazon      5   11\n",
       "1         MISC  0.990859  Optimus Prime     36   49\n",
       "2          LOC  0.999755        Germany     90   97\n",
       "3         MISC  0.556570           Mega    208  212\n",
       "4          PER  0.590255         ##tron    212  216\n",
       "5          ORG  0.669692         Decept    253  259\n",
       "6         MISC  0.498350        ##icons    259  264\n",
       "7         MISC  0.775362       Megatron    350  358\n",
       "8         MISC  0.987854  Optimus Prime    367  380\n",
       "9          PER  0.812096      Bumblebee    502  511"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create NER pipeline\n",
    "ner_tagger = pipeline(\"ner\", aggregation_strategy=\"simple\")\n",
    "outputs = ner_tagger(text)\n",
    "pd.DataFrame(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### ðŸ“š Question 3: Named Entity Recognition (NER)\n",
    "\n",
    "Let's understand NER better:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. What does the `aggregation_strategy=\"simple\"` parameter do in the NER pipeline? Check the [token classification documentation](https://huggingface.co/docs/transformers/main/en/task_summary#token-classification).\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "The `aggregation_strategy=\"simple\"` parameter:\n",
    "- **Groups together** consecutive tokens that belong to the same entity\n",
    "- **Simplifies output** by merging subword tokens (e.g., \"Optimus\" and \"Prime\" become \"Optimus Prime\")\n",
    "- **Averages scores** across the grouped tokens\n",
    "- Without it, you'd get separate entries for each token/subword\n",
    "\n",
    "#### 2. Looking at the output above, what do the entity types mean? (ORG, MISC, LOC, PER)\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "- **ORG**: Organization (companies, agencies, institutions) - e.g., \"Amazon\"\n",
    "- **MISC**: Miscellaneous (entities that don't fit other categories) - e.g., \"Optimus Prime\", \"Megatron\"\n",
    "- **LOC**: Location (geographical locations) - e.g., \"Germany\"\n",
    "- **PER**: Person (people's names) - e.g., \"Bumblebee\"\n",
    "\n",
    "#### 3. Why do some words appear with `##` prefix (like `##tron` and `##icons`)? What does this indicate about tokenization?\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "The `##` prefix indicates **subword tokenization**:\n",
    "- The model breaks unknown or rare words into smaller pieces (subwords)\n",
    "- `##` means \"this is a continuation of the previous token\"\n",
    "- Example: \"Megatron\" might be tokenized as [\"Mega\", \"##tron\"]\n",
    "- This allows the model to handle words it hasn't seen before by breaking them into known parts\n",
    "- It's a key feature of BERT-based tokenizers (WordPiece tokenization)\n",
    "\n",
    "#### 4. The model seems to have split \"Megatron\" and \"Decepticons\" incorrectly. Why might this happen? What does this tell you about the model's training data?\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "This happens because:\n",
    "- **Training data bias**: The model was likely trained on standard datasets (news articles, Wikipedia) that don't contain Transformers character names\n",
    "- **Out-of-vocabulary words**: \"Megatron\" and \"Decepticons\" are fictional names not in the training corpus\n",
    "- **Subword fallback**: The tokenizer breaks them into subwords it recognizes\n",
    "- **Domain mismatch**: The model performs well on formal text but struggles with pop culture references\n",
    "\n",
    "This tells us the model was trained on **formal, real-world text** (news, documents), not fictional or fan fiction content.\n",
    "\n",
    "#### 5. **Challenge**: Find the model card for `dbmdz/bert-large-cased-finetuned-conll03-english`. What is the CoNLL-2003 dataset?\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "The **CoNLL-2003 dataset** is:\n",
    "- A benchmark dataset for Named Entity Recognition\n",
    "- Contains news articles from Reuters corpus\n",
    "- Annotated with 4 entity types: PER, ORG, LOC, MISC\n",
    "- Widely used for training and evaluating NER models\n",
    "- Consists of English and German news text\n",
    "\n",
    "You can find it at: https://huggingface.co/datasets/conll2003"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ¤” **How might the choice of tokenizer affect NER performance?**\n",
    "\n",
    "Different tokenizers handle subwords differently, which can affect:\n",
    "- Entity boundary detection\n",
    "- Handling of rare/unknown words\n",
    "- Performance on domain-specific text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Question Answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-cased-distilled-squad and revision 564e9b5 (https://huggingface.co/distilbert/distilbert-base-cased-distilled-squad).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ef11c8c63204d04ab92479161945660",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/473 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f45ed987d85a4258a37edcdbfb43cbe1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/261M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb9f04a58124414782fea64b7077674c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6770b56330b44eda07aecdb1a7abda2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57f23e4aadc14853b26e8a1dfdf6293d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.631292</td>\n",
       "      <td>335</td>\n",
       "      <td>358</td>\n",
       "      <td>an exchange of Megatron</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      score  start  end                   answer\n",
       "0  0.631292    335  358  an exchange of Megatron"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create question-answering pipeline\n",
    "reader = pipeline(\"question-answering\")\n",
    "question = \"What does the customer want?\"\n",
    "outputs = reader(question=question, context=text)\n",
    "pd.DataFrame([outputs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### ðŸ“š Question 4: Question Answering Systems\n",
    "\n",
    "Explore how question answering works:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. What type of question answering is this? (Extractive vs. Generative) Check the [question answering documentation](https://huggingface.co/docs/transformers/main/en/task_summary#question-answering).\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "This is **Extractive Question Answering**:\n",
    "- The model **extracts** the answer directly from the provided context\n",
    "- It doesn't generate new text, only selects a span from the input\n",
    "- Returns `start` and `end` indices indicating where the answer is in the context\n",
    "\n",
    "**Generative QA** would create new text based on the context (like ChatGPT does).\n",
    "\n",
    "#### 2. The model outputs `start` and `end` indices. What do these represent? Why are they important?\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "- **`start`**: Character position where the answer begins in the context\n",
    "- **`end`**: Character position where the answer ends in the context\n",
    "- **Importance**: They allow you to:\n",
    "  - Extract the exact answer text from the context\n",
    "  - Highlight the answer in the original text\n",
    "  - Verify that the answer exists in the context\n",
    "  - Understand which part of the text the model focused on\n",
    "\n",
    "#### 3. What is the SQuAD dataset? (Look up the model `distilbert-base-cased-distilled-squad` on the Hub)\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "**SQuAD (Stanford Question Answering Dataset)**:\n",
    "- A reading comprehension dataset\n",
    "- Contains 100,000+ questions on Wikipedia articles\n",
    "- Each question has an answer that is a segment of text from the passage\n",
    "- Used to train and evaluate extractive question answering models\n",
    "- Two versions: SQuAD 1.1 and SQuAD 2.0 (includes unanswerable questions)\n",
    "\n",
    "#### 4. Try to think of a question this model CANNOT answer based on the text. Why would it fail?\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "Examples of questions it cannot answer:\n",
    "- \"What will Amazon do about this complaint?\" - Answer not in text (requires prediction)\n",
    "- \"Is Optimus Prime more expensive than Megatron?\" - Information not provided\n",
    "- \"How many Transformers toys has Bumblebee bought?\" - Requires inference/counting not in text\n",
    "- \"Why did the warehouse make this mistake?\" - Requires external knowledge\n",
    "\n",
    "**Why it fails**: \n",
    "- Extractive QA can **only** find answers explicitly stated in the text\n",
    "- Cannot infer, reason, or use external knowledge\n",
    "- Cannot answer questions requiring calculations or common sense\n",
    "\n",
    "#### 5. **Challenge**: What's the difference between extractive and generative question answering? Find an example of a generative QA model on the Hub.\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "**Extractive QA**:\n",
    "- Selects exact span from context\n",
    "- Fast and precise\n",
    "- Limited to text that exists in input\n",
    "- Example: BERT, DistilBERT\n",
    "\n",
    "**Generative QA**:\n",
    "- Generates new text as answer\n",
    "- Can rephrase and synthesize information\n",
    "- Can answer questions requiring reasoning\n",
    "- Can use multiple sources\n",
    "- Example models: `google/flan-t5-base`, `facebook/bart-large`, GPT models\n",
    "\n",
    "**Generative QA example models**:\n",
    "- `google/flan-t5-base`\n",
    "- `facebook/bart-large-cnn`\n",
    "- `t5-base` or `t5-large`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question the model CAN answer:\n",
      "Q: Who wrote this letter?\n",
      "A: Dear Amazon (score: 0.7421)\n",
      "\n",
      "Question the model CANNOT answer well:\n",
      "Q: What will Amazon do about this?\n",
      "A: Enclosed are copies of my records (score: 0.0452)\n",
      "Note: Low score indicates the model is uncertain because the answer isn't explicitly in the text\n"
     ]
    }
   ],
   "source": [
    "# Test questions the model CAN and CANNOT answer\n",
    "print(\"Question the model CAN answer:\")\n",
    "q1 = \"Who wrote this letter?\"\n",
    "result1 = reader(question=q1, context=text)\n",
    "print(f\"Q: {q1}\")\n",
    "print(f\"A: {result1['answer']} (score: {result1['score']:.4f})\")\n",
    "\n",
    "print(\"\\nQuestion the model CANNOT answer well:\")\n",
    "q2 = \"What will Amazon do about this?\"\n",
    "result2 = reader(question=q2, context=text)\n",
    "print(f\"Q: {q2}\")\n",
    "print(f\"A: {result2['answer']} (score: {result2['score']:.4f})\")\n",
    "print(\"Note: Low score indicates the model is uncertain because the answer isn't explicitly in the text\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ’¡ **Try asking questions that require reasoning or information not in the text. What happens?**\n",
    "\n",
    "The model will still try to extract something, but with a low confidence score!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ“š Question 5: Text Summarization\n",
    "\n",
    "Before running the summarization code, let's understand how it works:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. What is the difference between **extractive** and **abstractive** summarization? Check the [summarization documentation](https://huggingface.co/docs/transformers/main/en/task_summary#summarization).\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "**Extractive Summarization**:\n",
    "- Selects and copies important sentences/phrases from the original text\n",
    "- Does NOT generate new words\n",
    "- Faster and simpler\n",
    "- Example: Selecting the 3 most important sentences from an article\n",
    "\n",
    "**Abstractive Summarization**:\n",
    "- Generates new sentences that capture the meaning\n",
    "- Can paraphrase and use different words\n",
    "- More human-like but computationally expensive\n",
    "- Example: Most Hugging Face summarization models (BART, T5, Pegasus)\n",
    "\n",
    "#### 2. Looking at the code in the next cell, what is the default model used for summarization? Search for it on the [Hugging Face Model Hub](https://huggingface.co/models) and determine:\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "The default model is: **`sshleifer/distilbart-cnn-12-6`**\n",
    "\n",
    "- **Is it an extractive or abstractive model?** Abstractive (based on BART architecture)\n",
    "- **What architecture does it use?** DistilBART (distilled version of BART - Bidirectional and Auto-Regressive Transformer)\n",
    "- **What dataset was it trained on?** CNN/DailyMail dataset (news articles and summaries)\n",
    "\n",
    "#### 3. What do the `max_length` and `min_length` parameters control? What happens if `min_length` > `max_length`?\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "- **`max_length`**: Maximum number of tokens in the generated summary\n",
    "- **`min_length`**: Minimum number of tokens in the generated summary\n",
    "- **If `min_length` > `max_length`**: The model will throw an error or produce unexpected behavior because it's an impossible constraint\n",
    "\n",
    "These parameters help control summary length:\n",
    "- Too short: May miss important information\n",
    "- Too long: May include unnecessary details\n",
    "\n",
    "#### 4. The parameter `clean_up_tokenization_spaces=True` is used. What does this parameter do? Why might it be useful for summarization?\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "`clean_up_tokenization_spaces=True`:\n",
    "- Removes extra spaces that might appear during tokenization/detokenization\n",
    "- Fixes spacing around punctuation (e.g., \"word .\" â†’ \"word.\")\n",
    "- Makes the output more readable and natural\n",
    "\n",
    "**Why useful for summarization?**\n",
    "- Summaries need to be clean and professional\n",
    "- Tokenizers can add spaces in unexpected places\n",
    "- Improves presentation quality of the final output\n",
    "\n",
    "#### 5. **Challenge**: Find two different summarization models on the Hub:\n",
    "   - One optimized for short texts (like news articles)\n",
    "   - One that can handle longer documents\n",
    "   \n",
    "   Compare their architectures and training data.\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "**For short texts (news articles)**:\n",
    "- Model: `facebook/bart-large-cnn` or `sshleifer/distilbart-cnn-12-6`\n",
    "- Architecture: BART (encoder-decoder)\n",
    "- Training data: CNN/DailyMail dataset (news articles ~400-800 words)\n",
    "- Good for: News, blog posts, short documents\n",
    "\n",
    "**For longer documents**:\n",
    "- Model: `google/pegasus-large` or `facebook/bart-large-xsum`\n",
    "- Architecture: Pegasus (designed for long documents) or BART\n",
    "- Training data: Larger datasets with longer documents\n",
    "- Good for: Research papers, reports, books\n",
    "- Alternative: `pszemraj/led-base-book-summary` (LED = Longformer Encoder-Decoder, can handle 16K tokens)\n",
    "\n",
    "**Key differences**:\n",
    "- Short-text models: Faster, trained on concise summaries\n",
    "- Long-document models: Can handle more context but slower and require more memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ¤” **Why might summarization be more challenging than text classification? What linguistic capabilities does the model need?**\n",
    "\n",
    "Summarization requires:\n",
    "- Understanding overall meaning and key points\n",
    "- Generating coherent new text\n",
    "- Paraphrasing and compression skills\n",
    "- Maintaining factual accuracy\n",
    "- Proper grammar and flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df0180bfd0614fb29d3d33ae2065441a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f0d79c8686b41d190ddc3cdb6eec1dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/1.22G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f85b1afca31d4e369e9974ebebf96c1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.22G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d543a9b79d54d5eb37fcc5d731e1750",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a92b9ef8513f46ef9c2edfa3a2871969",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de2bb22e078643488f5d2755e4791978",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Your min_length=56 must be inferior than your max_length=45.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text length: 512\n",
      "Summary length: 211\n",
      "\n",
      "Summary:\n",
      " Bumblebee ordered an Optimus Prime action figure from your online store in Germany. Unfortunately, when I opened the package, I discovered to my horror that I had been sent an action figure of Megatron instead.\n"
     ]
    }
   ],
   "source": [
    "# Create summarization pipeline\n",
    "summarizer = pipeline(\"summarization\")\n",
    "outputs = summarizer(text, max_length=45, clean_up_tokenization_spaces=True)\n",
    "print(\"Original text length:\", len(text))\n",
    "print(\"Summary length:\", len(outputs[0]['summary_text']))\n",
    "print(\"\\nSummary:\")\n",
    "print(outputs[0]['summary_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparing different summary lengths:\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your min_length=56 must be inferior than your max_length=30.\n",
      "Your min_length=56 must be inferior than your max_length=45.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length 30:\n",
      " Bumblebee ordered an Optimus Prime action figure from your online store in Germany. Unfortunately, when I opened the package, I discovered to\n",
      "\n",
      "Max length 45:\n",
      " Bumblebee ordered an Optimus Prime action figure from your online store in Germany. Unfortunately, when I opened the package, I discovered to my horror that I had been sent an action figure of Megatron instead.\n",
      "\n",
      "Max length 60:\n",
      " Bumblebee ordered an Optimus Prime action figure from your online store in Germany. Unfortunately, when I opened the package, I discovered to my horror that I had been sent an action figure of Megatron instead. As a lifelong enemy of the Decepticons, I hope you can understand\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's compare different summary lengths\n",
    "print(\"Comparing different summary lengths:\\n\")\n",
    "\n",
    "for max_len in [30, 45, 60]:\n",
    "    output = summarizer(text, max_length=max_len, clean_up_tokenization_spaces=True)\n",
    "    print(f\"Max length {max_len}:\")\n",
    "    print(output[0]['summary_text'])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Machine Translation\n",
    "\n",
    "### ðŸ“š Question 6: Machine Translation\n",
    "\n",
    "Let's explore how translation models work:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. What is the architecture behind the `Helsinki-NLP/opus-mt-en-de` model? Look it up on the [Model Hub](https://huggingface.co/Helsinki-NLP/opus-mt-en-de).\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "Architecture: **MarianMT (Marian Neural Machine Translation)**\n",
    "\n",
    "Key features:\n",
    "- **What does \"OPUS\" stand for?** OPUS = Open Parallel Corpus (a collection of translated texts)\n",
    "- **What does \"MT\" stand for?** MT = Machine Translation\n",
    "- **Architecture type**: Transformer-based encoder-decoder model\n",
    "- **Specialized for**: Neural machine translation\n",
    "- **Training**: Trained on parallel corpora (aligned texts in multiple languages)\n",
    "\n",
    "#### 2. How would you find a model to translate from English to French? Visit the [translation documentation](https://huggingface.co/docs/transformers/main/en/task_summary#translation) and the Model Hub to find at least 2 different models.\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "To find English to French models:\n",
    "1. Go to Hugging Face Model Hub\n",
    "2. Filter by task: \"Translation\"\n",
    "3. Search for \"en-fr\" or \"English to French\"\n",
    "\n",
    "**Example models**:\n",
    "- `Helsinki-NLP/opus-mt-en-fr` - Standard MarianMT model\n",
    "- `facebook/mbart-large-50-many-to-many-mmt` - Multilingual model\n",
    "- `t5-base` with task prefix \"translate English to French:\"\n",
    "\n",
    "#### 3. What is the difference between **bilingual** and **multilingual** translation models? What are the advantages and disadvantages of each?\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "**Bilingual Models** (e.g., `Helsinki-NLP/opus-mt-en-de`):\n",
    "- Translate between **only 2 languages** (e.g., English â†” German)\n",
    "- **Advantages**:\n",
    "  - Higher quality for that specific language pair\n",
    "  - Faster inference\n",
    "  - Smaller model size\n",
    "- **Disadvantages**:\n",
    "  - Need separate model for each language pair\n",
    "  - Cannot leverage cross-lingual knowledge\n",
    "\n",
    "**Multilingual Models** (e.g., `facebook/mbart-large-50`):\n",
    "- Translate between **multiple languages** (50+ languages)\n",
    "- **Advantages**:\n",
    "  - One model for many language pairs\n",
    "  - Can do zero-shot translation (pairs not seen in training)\n",
    "  - Transfers knowledge across languages\n",
    "- **Disadvantages**:\n",
    "  - Lower quality per language pair\n",
    "  - Larger model size\n",
    "  - Slower inference\n",
    "\n",
    "#### 4. In the code, we specify the task as `\"translation_en_to_de\"`. How does this relate to the model we're loading?\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "- The task string `\"translation_en_to_de\"` tells the pipeline what type of translation to perform\n",
    "- It must **match** the model's capabilities (enâ†’de)\n",
    "- The pipeline uses this to:\n",
    "  - Select appropriate preprocessing\n",
    "  - Configure the tokenizer correctly\n",
    "  - Set up the correct source and target languages\n",
    "- If you specified `\"translation_en_to_fr\"` with an en-de model, it would fail or produce wrong results\n",
    "\n",
    "#### 5. The output shows a warning about `sacremoses`. What is this library used for in NLP? Check the [MarianMT documentation](https://huggingface.co/docs/transformers/model_doc/marian).\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "**Sacremoses**:\n",
    "- A Python library for **tokenization** and **text processing**\n",
    "- Originally from Moses Statistical Machine Translation system\n",
    "- Used for:\n",
    "  - Tokenizing text before translation\n",
    "  - Detokenizing translation output\n",
    "  - Handling punctuation and special characters\n",
    "  - Normalizing text (e.g., quotes, spaces)\n",
    "- MarianMT models use it for preprocessing input and postprocessing output\n",
    "- The warning appears if sacremoses isn't installed (can fall back to basic tokenization)\n",
    "\n",
    "#### 6. **Challenge**: Find a multilingual model (like mBART or M2M100) that can translate between multiple language pairs. How many language pairs does it support?\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "**Example: `facebook/mbart-large-50-many-to-many-mmt`**\n",
    "- Supports **50 languages**\n",
    "- Can translate between **50 Ã— 49 = 2,450 language pairs** (any language to any other)\n",
    "- Languages include: English, French, German, Spanish, Chinese, Arabic, Japanese, etc.\n",
    "\n",
    "**Example: `facebook/m2m100_418M`**\n",
    "- Supports **100 languages**\n",
    "- Can translate between **100 Ã— 99 = 9,900 language pairs**\n",
    "- First model to translate directly between 100 languages without English as intermediate\n",
    "\n",
    "**Example: `google/flan-t5-base`** with task prefix:\n",
    "- Can translate many language pairs using task instructions\n",
    "- More flexible but may have lower quality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸŒ **What challenges exist for low-resource languages?**\n",
    "\n",
    "Low-resource languages face:\n",
    "- Less training data available\n",
    "- Lower translation quality\n",
    "- Fewer parallel corpora\n",
    "- Need for multilingual models to transfer knowledge from high-resource languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16632912c7db47e195fdedcaeed6c127",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "145f39c458374f0c8d2c1b6d03f19d12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/298M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8bc16b12cce4748a7b100f77df94fd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/298M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5e2e935e5c84bb39b2b1d0aa90f1b07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/293 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "591106cb54504a8bbf7036728a581bd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/42.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2b1d5b2b6d241c09db461fc758cda56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "source.spm:   0%|          | 0.00/768k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ad98409346d462b80ac64881fe6e4ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "target.spm:   0%|          | 0.00/797k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a290210f3704a0aa9ca471b2d9f8fae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "German translation:\n",
      "Sehr geehrter Amazon, letzte Woche habe ich eine Optimus Prime Action Figur aus Ihrem Online-Shop in Deutschland bestellt. Leider, als ich das Paket Ã¶ffnete, entdeckte ich zu meinem Entsetzen, dass ich stattdessen eine Action Figur von Megatron geschickt worden war! Als lebenslanger Feind der Decepticons, Ich hoffe, Sie kÃ¶nnen mein Dilemma verstehen. Um das Problem zu lÃ¶sen, Ich fordere einen Austausch von Megatron fÃ¼r die Optimus Prime Figur habe ich bestellt. Eingeschlossen sind Kopien meiner Aufzeichnungen Ã¼ber diesen Kauf. Ich erwarte, von Ihnen bald zu hÃ¶ren. Aufrichtig, Bumblebee.\n"
     ]
    }
   ],
   "source": [
    "# Create translation pipeline (English to German)\n",
    "translator = pipeline(\"translation_en_to_de\", model=\"Helsinki-NLP/opus-mt-en-de\")\n",
    "outputs = translator(text, clean_up_tokenization_spaces=True, min_length=100)\n",
    "print(\"German translation:\")\n",
    "print(outputs[0]['translation_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ffc0852cf474fdb9d796b830e1b6b9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab745d4bac834b1fa74f0141a79a895d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/301M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b97785fc45b44f2afa4eedd95260bab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/301M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9b20c10b3d2488b9a4923f213e756f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/293 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b664a2dcd9de4cf4a19c51faad5d8058",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/42.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9e3e68dd07c48c6aaad09a26a29950d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "source.spm:   0%|          | 0.00/778k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdf5244286c54d3fa687aa9afce193c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "target.spm:   0%|          | 0.00/802k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f03dc78bad654d0784d27dfce9bca6e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "French translation:\n",
      "Cher Amazon, la semaine derniÃ¨re j'ai commandÃ© une figure d'action Optimus Prime de votre boutique en ligne en Allemagne. Malheureusement, quand j'ai ouvert le paquet, j'ai dÃ©couvert Ã  mon horreur que j'avais Ã©tÃ© envoyÃ© une figure d'action de Megatron Ã  la place! En tant qu'ennemi Ã  vie des Decepticons, j'espÃ¨re que vous pouvez comprendre mon dilemme. Pour rÃ©soudre le problÃ¨me, j'exige un Ã©change de Megatron contre la figure d'Optimus Prime que j'ai commandÃ©.\n"
     ]
    }
   ],
   "source": [
    "# Let's try translating to French as well\n",
    "translator_fr = pipeline(\"translation_en_to_fr\", model=\"Helsinki-NLP/opus-mt-en-fr\")\n",
    "outputs_fr = translator_fr(text, clean_up_tokenization_spaces=True, min_length=100)\n",
    "print(\"French translation:\")\n",
    "print(outputs_fr[0]['translation_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparing bilingual vs multilingual translation:\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bilingual (Helsinki-NLP):\n",
      "Hallo, wie geht's?\n",
      "\n",
      "Multilingual models like mBART can translate between 50+ languages\n",
      "but require different setup with language codes\n"
     ]
    }
   ],
   "source": [
    "# Compare bilingual vs multilingual model\n",
    "print(\"Comparing bilingual vs multilingual translation:\\n\")\n",
    "\n",
    "# Bilingual model (en-de only)\n",
    "bilingual_model = pipeline(\"translation_en_to_de\", model=\"Helsinki-NLP/opus-mt-en-de\")\n",
    "bilingual_output = bilingual_model(\"Hello, how are you?\", clean_up_tokenization_spaces=True)\n",
    "print(\"Bilingual (Helsinki-NLP):\")\n",
    "print(bilingual_output[0]['translation_text'])\n",
    "print()\n",
    "\n",
    "# Note: For multilingual models like mBART, you would need to specify source and target languages differently\n",
    "print(\"Multilingual models like mBART can translate between 50+ languages\")\n",
    "print(\"but require different setup with language codes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Text Generation\n",
    "\n",
    "### ðŸ“š Question 7: Text Generation\n",
    "\n",
    "Understand how language models generate text:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. What is the default model used for text generation in the code below? Look it up on the Hub and answer:\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "The default model is: **`gpt2`** (specifically `openai-community/gpt2`)\n",
    "\n",
    "- **What architecture does GPT-2 use?** \n",
    "  - Decoder-only Transformer (autoregressive language model)\n",
    "  - Uses masked self-attention to generate text left-to-right\n",
    "\n",
    "- **How many parameters does the base GPT-2 model have?**\n",
    "  - GPT-2 small: **117M parameters**\n",
    "  - GPT-2 medium: 345M parameters\n",
    "  - GPT-2 large: 774M parameters\n",
    "  - GPT-2 XL: 1.5B parameters\n",
    "\n",
    "- **What type of generation does it perform?** \n",
    "  - **Autoregressive generation** (generates one token at a time)\n",
    "  - Non-autoregressive would generate all tokens simultaneously (rare)\n",
    "\n",
    "#### 2. Why do we use `set_seed(42)` before generation? What would happen without it? Check the [generation documentation](https://huggingface.co/docs/transformers/main/en/main_classes/text_generation).\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "`set_seed(42)` sets the random seed for reproducibility:\n",
    "\n",
    "**With seed**:\n",
    "- Same input â†’ Same output every time\n",
    "- Reproducible results for debugging and demos\n",
    "- Important for research and testing\n",
    "\n",
    "**Without seed**:\n",
    "- Same input â†’ Different output each time\n",
    "- Uses random sampling for token selection\n",
    "- More diverse but unpredictable\n",
    "\n",
    "The number 42 is arbitrary (often used as a reference to \"The Hitchhiker's Guide to the Galaxy\")\n",
    "\n",
    "#### 3. The code uses `max_length=200`. What other parameters can control text generation? Research and explain:\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "Key generation parameters:\n",
    "\n",
    "**`temperature` (0.0 to 2.0)**:\n",
    "- Controls randomness in token selection\n",
    "- Lower (0.1-0.7): More focused, deterministic, repetitive\n",
    "- Higher (0.8-1.5): More creative, diverse, potentially incoherent\n",
    "- Default: 1.0\n",
    "\n",
    "**`top_k` (integer)**:\n",
    "- Considers only top K most likely tokens\n",
    "- Lower (10-20): More focused vocabulary\n",
    "- Higher (50-100): More diverse word choice\n",
    "- Prevents sampling very unlikely tokens\n",
    "\n",
    "**`do_sample` (True/False)**:\n",
    "- If True: Use sampling (random selection based on probabilities)\n",
    "- If False: Use greedy decoding (always pick most likely token)\n",
    "- Must be True to use temperature, top_k, top_p\n",
    "\n",
    "Other parameters:\n",
    "- `top_p` (nucleus sampling): Cumulative probability threshold\n",
    "- `num_beams`: Beam search for better quality\n",
    "- `repetition_penalty`: Penalize repeating tokens\n",
    "- `length_penalty`: Control output length preference\n",
    "\n",
    "#### 4. Looking at the output, you can see a warning about truncation. What does this mean? Why is the input being truncated?\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "**Truncation** occurs when:\n",
    "- The input text is **too long** for the model's maximum context window\n",
    "- GPT-2 has a **maximum sequence length of 1024 tokens**\n",
    "- If input + `max_length` exceeds this limit, input gets cut off\n",
    "\n",
    "**Why it happens**:\n",
    "- Models have fixed context window sizes (architectural limitation)\n",
    "- Longer contexts require exponentially more memory and computation\n",
    "- In our example: input (complaint letter) + max_length=200 might exceed limits\n",
    "\n",
    "**Solution**: \n",
    "- Use shorter input\n",
    "- Reduce `max_length`\n",
    "- Use models with larger context (e.g., GPT-3, Claude, GPT-4)\n",
    "\n",
    "#### 5. What does `pad_token_id` being set to `eos_token_id` mean? Why is this necessary for GPT-2?\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "**Explanation**:\n",
    "- **`pad_token_id`**: Special token used to pad sequences to equal length in batches\n",
    "- **`eos_token_id`**: Special token indicating \"end of sequence\"\n",
    "- GPT-2 was **not originally trained with a padding token**\n",
    "\n",
    "**Why necessary**:\n",
    "- When processing batches, sequences need to be same length\n",
    "- Without a dedicated pad token, GPT-2 would treat padding as real content\n",
    "- Setting `pad_token_id = eos_token_id` tells the model to ignore padding\n",
    "- This is a common workaround for GPT-2 and similar models\n",
    "\n",
    "**Alternative**: Some newer models (like GPT-3, LLaMA) have dedicated padding tokens\n",
    "\n",
    "#### 6. What are the trade-offs between model size and generation quality?\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "**Larger Models** (GPT-2 XL, GPT-3, GPT-4):\n",
    "- âœ… Better coherence and fluency\n",
    "- âœ… Better understanding of context\n",
    "- âœ… More factual accuracy\n",
    "- âœ… Better following of instructions\n",
    "- âŒ Slower inference time\n",
    "- âŒ More memory required (GB to TB)\n",
    "- âŒ Higher computational cost\n",
    "- âŒ More expensive to run\n",
    "\n",
    "**Smaller Models** (GPT-2 small, DistilGPT-2):\n",
    "- âœ… Fast inference\n",
    "- âœ… Low memory usage\n",
    "- âœ… Can run on consumer hardware\n",
    "- âœ… Cheaper to deploy\n",
    "- âŒ Less coherent over long texts\n",
    "- âŒ More repetition\n",
    "- âŒ Less factual knowledge\n",
    "- âŒ Simpler vocabulary\n",
    "\n",
    "**General principle**: Quality scales with parameter count, but with diminishing returns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ’­ **What ethical considerations exist for text generation models?**\n",
    "\n",
    "Important concerns:\n",
    "- **Misinformation**: Can generate false but convincing text\n",
    "- **Bias**: Reflects biases in training data\n",
    "- **Misuse**: Could be used for spam, fake reviews, impersonation\n",
    "- **Attribution**: Who owns AI-generated content?\n",
    "- **Detection**: How to identify AI-generated text?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to openai-community/gpt2 and revision 607a30d (https://huggingface.co/openai-community/gpt2).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "239d1be3b0a845a8b07e76f78ac6e8b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b4518abefb7424a818ab195ac534727",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d6d250c10734d8d974ccf8a59cb213a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5673546fa99747998bf28efcfd11bd99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8cb8bf185c54c5b89b6f61608802527",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a178ce6bb47c424bb5b009e7024fd62f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3050327785d249b89eb62db6ba401d80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=200) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text:\n",
      "Dear Amazon, last week I ordered an Optimus Prime action figure from your online store in Germany. Unfortunately, when I opened the package, I discovered to my horror that I had been sent an action figure of Megatron instead! As a lifelong enemy of the Decepticons, I hope you can understand my dilemma. To resolve the issue, I demand an exchange of Megatron for the Optimus Prime figure I ordered. Enclosed are copies of my records concerning this purchase. I expect to hear from you soon. Sincerely, Bumblebee.\n",
      "\n",
      "Customer service response:\n",
      "Dear Bumblebee, I am sorry to hear that your order was mixed up. I would like to know if you know more about our service. Please let me know if we can arrange an exchange of Megatron for you.\n",
      "\n",
      "The following quote from my customer service representative is from my review of the Optimus Prime action figure:\n",
      "\n",
      "\"Hi. I was a bit stunned when I saw the Optimus Prime action figure from your online store. I was hoping you could make me happy, but I was not able to find a copy of this figure. All I can do is ask that you please keep this figure from going out of stock at your local game store or another retailer.\"\n",
      "\n",
      "I agree with your post and am glad that you were able to resolve the issue with my order.\n",
      "\n",
      "The following quote from my customer service representative is from my review of the Optimus Prime action figure:\n",
      "\n",
      "\"Hi. I was a bit stunned when I saw the Optimus Prime action figure from your online store. I was hoping you could make me happy, but I was not able to find a copy of this figure. All I can do is ask that you please keep this figure from going out of stock at your local game store or another retailer.\"\n",
      "\n",
      "Thanks for your review, I am just as upset about your store being overwhelmed with the Transformers action\n"
     ]
    }
   ],
   "source": [
    "# Set seed for reproducibility\n",
    "set_seed(42)\n",
    "\n",
    "# Create text generation pipeline\n",
    "generator = pipeline(\"text-generation\")\n",
    "\n",
    "# Create a prompt for generation\n",
    "response = \"Dear Bumblebee, I am sorry to hear that your order was mixed up.\"\n",
    "prompt = text + \"\\n\\nCustomer service response:\\n\" + response\n",
    "\n",
    "# Generate continuation\n",
    "outputs = generator(prompt, max_length=200)\n",
    "print(\"Generated text:\")\n",
    "print(outputs[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparing different generation strategies:\n",
      "\n",
      "1. Greedy decoding (do_sample=False):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " I have received your order and am happy to exchange it for the Optimus Prime action figure. I have also received your order and am happy to exchange it for the Optimus Prime action figure. I have also received your order and am happy to exchange it for the Optimus Prime action figure. I have also received your order and am happy to exchange it for the Optimus Prime action figure. I have also received your order and am happy to exchange it for the Optimus Prime action figure. I have also received your order and am happy to exchange it for the Optimus Prime action figure. I have also received your order and am happy to exchange it for the Optimus Prime action figure. I have also received your order and am happy to exchange it for the Optimus Prime action figure. I have also received your order and am happy to exchange it for the Optimus Prime action figure. I have also received your order and am happy to exchange it for the Optimus Prime action figure. I have also received your order and am happy to exchange it for the Optimus Prime action figure. I have also received your order and am happy to exchange it for the Optimus Prime action figure. I have also received your order and am happy to exchange it for the Optimus Prime action figure. I have also received your order and am happy to\n",
      "\n",
      "2. Sampling with temperature=0.7:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " I would like to know if you know more about our service. Please let me know if we can arrange an exchange of Megatron for you.\n",
      "\n",
      "The following quote from my customer service representative is from my review of the Optimus Prime action figure:\n",
      "\n",
      "\"Hi. I was a bit stunned when I saw the Optimus Prime action figure from your online store. I was hoping you could make me happy, but I was not able to find a copy of this figure. All I can do is ask that you please keep this figure from going out of stock at your local game store or another retailer.\"\n",
      "\n",
      "I agree with your post and am glad that you were able to resolve the issue with my order.\n",
      "\n",
      "The following quote from my customer service representative is from my review of the Optimus Prime action figure:\n",
      "\n",
      "\"Hi. I was a bit stunned when I saw the Optimus Prime action figure from your online store. I was hoping you could make me happy, but I was not able to find a copy of this figure. All I can do is ask that you please keep this figure from going out of stock at your local game store or another retailer.\"\n",
      "\n",
      "Thanks for your review, I am just as upset about your store being overwhelmed with the Transformers action\n",
      "\n",
      "3. Top-k sampling (k=50):\n",
      " I would like to know if you know more about our service. Please let me know if we can arrange an exchange of Megatron for you.\n",
      "\n",
      "The following quote from my customer service representative is from my review of the Optimus Prime action figure:\n",
      "\n",
      "\"Hi. I was a bit stunned when I saw the Optimus Prime action figure from your online store. I was hoping you could make me happy, but I was not able to find a copy of this figure. All I can do is ask that you please keep this figure from going out of stock at your local game store or another retailer.\"\n",
      "\n",
      "I agree with your post and am glad that you were able to resolve the issue with my order.\n",
      "\n",
      "The following quote from my customer service representative is from my review of the Optimus Prime action figure:\n",
      "\n",
      "\"Hi. I was a bit stunned when I saw the Optimus Prime action figure from your online store. I was hoping you could make me happy, but I was not able to find a copy of this figure. All I can do is ask that you please keep this figure from going out of stock at your local game store or another retailer.\"\n",
      "\n",
      "Thanks for your review, I am just as upset about your store being overwhelmed with the Transformers action\n"
     ]
    }
   ],
   "source": [
    "# Experiment with different generation parameters\n",
    "print(\"Comparing different generation strategies:\\n\")\n",
    "\n",
    "# Greedy (deterministic)\n",
    "print(\"1. Greedy decoding (do_sample=False):\")\n",
    "outputs_greedy = generator(prompt, max_length=150, do_sample=False, pad_token_id=generator.tokenizer.eos_token_id)\n",
    "print(outputs_greedy[0]['generated_text'][len(prompt):])\n",
    "print()\n",
    "\n",
    "# Sampling with temperature\n",
    "print(\"2. Sampling with temperature=0.7:\")\n",
    "set_seed(42)\n",
    "outputs_temp = generator(prompt, max_length=150, do_sample=True, temperature=0.7, pad_token_id=generator.tokenizer.eos_token_id)\n",
    "print(outputs_temp[0]['generated_text'][len(prompt):])\n",
    "print()\n",
    "\n",
    "# Top-k sampling\n",
    "print(\"3. Top-k sampling (k=50):\")\n",
    "set_seed(42)\n",
    "outputs_topk = generator(prompt, max_length=150, do_sample=True, top_k=50, pad_token_id=generator.tokenizer.eos_token_id)\n",
    "print(outputs_topk[0]['generated_text'][len(prompt):])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary and Conclusions\n",
    "\n",
    "### What we learned:\n",
    "\n",
    "1. **Pipelines** make it easy to use pre-trained models without dealing with tokenization details\n",
    "\n",
    "2. **Text Classification** (sentiment analysis) can classify text into categories with confidence scores\n",
    "\n",
    "3. **Named Entity Recognition (NER)** identifies and categorizes entities (people, organizations, locations)\n",
    "\n",
    "4. **Question Answering** can extract answers from context (extractive) or generate answers (generative)\n",
    "\n",
    "5. **Summarization** can condense long texts while preserving key information\n",
    "\n",
    "6. **Translation** models can convert text between languages (bilingual or multilingual)\n",
    "\n",
    "7. **Text Generation** creates new text continuations using autoregressive models\n",
    "\n",
    "### Key Takeaways:\n",
    "\n",
    "- **Model selection matters**: Different models are trained on different data and excel at different tasks\n",
    "- **Parameters control behavior**: Temperature, top_k, max_length, etc. significantly affect outputs\n",
    "- **Trade-offs exist**: Quality vs. speed, specificity vs. generality, extractive vs. generative\n",
    "- **Context matters**: Models perform best on text similar to their training data\n",
    "- **Always check the Model Hub**: Read model cards to understand capabilities and limitations\n",
    "\n",
    "### Important Concepts:\n",
    "\n",
    "- **Tokenization**: Breaking text into subwords (handles unknown words)\n",
    "- **Transfer Learning**: Pre-trained models fine-tuned for specific tasks\n",
    "- **Confidence Scores**: Model's certainty about predictions (0-1)\n",
    "- **Extractive vs. Generative**: Selecting from input vs. creating new text\n",
    "- **Autoregressive**: Generating one token at a time based on previous tokens\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "- Explore more models on the [Hugging Face Hub](https://huggingface.co/models)\n",
    "- Try fine-tuning models on your own data\n",
    "- Learn about model architectures (BERT, GPT, T5, etc.)\n",
    "- Understand evaluation metrics for different tasks\n",
    "- Consider ethical implications of deployed models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Resources\n",
    "\n",
    "- ðŸ“š [Hugging Face Documentation](https://huggingface.co/docs/transformers)\n",
    "- ðŸŽ“ [Hugging Face Course](https://huggingface.co/course)\n",
    "- ðŸ¤— [Model Hub](https://huggingface.co/models)\n",
    "- ðŸ“Š [Datasets Hub](https://huggingface.co/datasets)\n",
    "- ðŸ’¬ [Community Forums](https://discuss.huggingface.co/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
