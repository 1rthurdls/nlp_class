{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification with Generative Models\n",
    "\n",
    "Text classification is a common task in NLP. It involves categorizing text into predefined categories or classes based on its content. This task is essential in various applications, such as sentiment analysis, spam filtering, topic classification...\n",
    "\n",
    "![Text Classification Meme](https://i.imgflip.com/7xqz0v.jpg)\n",
    "\n",
    "Now with all the generative models it's tempting to use them for classification tasks, but are they good at it? How can we measure the success of a classification model? Let's find out ü§î\n",
    "\n",
    "For this example we will use the `rotten_tomatoes` dataset, it contains 50000 movie reviews with their corresponding sentiment (positive or negative)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies using uv\n",
    "import sys\n",
    "!{sys.executable} -m pip install uv\n",
    "!uv pip install transformers==4.41.2 accelerate==0.31.0 torch datasets sentence-transformers scikit-learn pandas numpy groq python-dotenv --system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "from datasets import load_dataset\n",
    "from transformers import pipeline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load our data\n",
    "data = load_dataset(\"rotten_tomatoes\")\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Using a Task-Specific Model\n",
    "\n",
    "Using specific task models is the easiest way to solve our problem, we just need to find a model that fits our needs, download it and use it in a pipeline to test it on our data.\n",
    "\n",
    "> For this example we will use a `roberta` model to classify our data.\n",
    "\n",
    "We will use a `pipeline` object. If you are not familiar with this, read the [official doc](https://huggingface.co/docs/transformers/main/en/pipeline_tutorial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìö Questions: Using Task-Specific Models\n",
    "\n",
    "Before running the code, let's understand what we're doing:\n",
    "\n",
    "#### 1. What is RoBERTa?\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "**RoBERTa** (Robustly Optimized BERT Approach) is an improved version of BERT:\n",
    "\n",
    "- **Architecture**: Same as BERT (bidirectional transformer encoder)\n",
    "- **Key improvements over BERT**:\n",
    "  - Trained on **more data** (160GB vs 16GB)\n",
    "  - Trained **longer** with **larger batches**\n",
    "  - Removed Next Sentence Prediction (NSP) task\n",
    "  - Dynamic masking (changes which tokens are masked during training)\n",
    "  - Uses byte-pair encoding (BPE) instead of WordPiece\n",
    "- **Result**: Better performance on most NLP benchmarks\n",
    "- **Use cases**: Text classification, NER, question answering, etc.\n",
    "\n",
    "#### 2. What does \"cardiffnlp/twitter-roberta-base-sentiment-latest\" tell us about this model?\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "Breaking down the model name:\n",
    "- **`cardiffnlp`**: Organization/creator (Cardiff NLP research group)\n",
    "- **`twitter-roberta-base`**: RoBERTa base model trained on Twitter data\n",
    "- **`sentiment-latest`**: Fine-tuned for sentiment analysis (latest version)\n",
    "\n",
    "**Key characteristics**:\n",
    "- Trained on **Twitter text** ‚Üí good for informal, social media language\n",
    "- Specialized for **sentiment analysis**\n",
    "- May perform well on movie reviews (similar informal style)\n",
    "- Understands abbreviations, slang, emojis common in social media\n",
    "\n",
    "#### 3. Why do we use `return_all_scores=True`?\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "- **Without this parameter**: Returns only the highest-scoring label\n",
    "- **With `return_all_scores=True`**: Returns confidence scores for ALL classes\n",
    "\n",
    "**Benefits**:\n",
    "- See model's confidence for each class (negative vs positive)\n",
    "- Understand how certain/uncertain the model is\n",
    "- Useful for setting custom thresholds\n",
    "- Better for debugging and analysis\n",
    "\n",
    "Example:\n",
    "```python\n",
    "# Without return_all_scores: {'label': 'POSITIVE', 'score': 0.95}\n",
    "# With return_all_scores: [{'label': 'NEGATIVE', 'score': 0.05}, {'label': 'POSITIVE', 'score': 0.95}]\n",
    "```\n",
    "\n",
    "#### 4. What does `device=\"cuda\"` do? What if you don't have a GPU?\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "**`device=\"cuda\"`**:\n",
    "- Runs the model on **GPU** (Graphics Processing Unit)\n",
    "- **Much faster** inference (10-100x speedup for large models)\n",
    "- Required for processing large datasets efficiently\n",
    "\n",
    "**If you don't have a GPU**:\n",
    "- Use `device=\"cpu\"` or omit the parameter (defaults to CPU)\n",
    "- Or use `device=-1` (also means CPU)\n",
    "- Inference will be slower but still work\n",
    "- Consider using smaller models or processing in smaller batches\n",
    "\n",
    "**Check if you have CUDA available**:\n",
    "```python\n",
    "import torch\n",
    "print(torch.cuda.is_available())  # True if GPU available\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "model_path = \"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load model into pipeline\n",
    "pipe = pipeline(\n",
    "    model=model_path,\n",
    "    tokenizer=model_path,\n",
    "    return_all_scores=True,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's run an inference loop to get the predictions for our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from transformers.pipelines.pt_utils import KeyDataset\n",
    "\n",
    "# Run inference\n",
    "y_pred = []\n",
    "for output in tqdm(pipe(KeyDataset(data[\"test\"], \"text\")), total=len(data[\"test\"])):\n",
    "    negative_score = output[0]['score']\n",
    "    positive_score = output[2]['score']  # Note: index 2, not 1 (neutral is at index 1)\n",
    "    assignment = np.argmax([negative_score, positive_score])\n",
    "    y_pred.append(assignment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Evaluation\n",
    "\n",
    "Then we will define a function to evaluate how well the model performed by comparing predictions to actual labels. For this we will use the `classification_report` from sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìö Questions: Understanding Evaluation Metrics\n",
    "\n",
    "#### 1. What is a classification report?\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "A **classification report** is a summary of a classifier's performance showing:\n",
    "\n",
    "- **Precision**: Of all items predicted as a class, how many were correct?\n",
    "  - Formula: TP / (TP + FP)\n",
    "  - \"When the model says positive, how often is it right?\"\n",
    "\n",
    "- **Recall**: Of all actual items in a class, how many did we find?\n",
    "  - Formula: TP / (TP + FN)\n",
    "  - \"Of all positive reviews, how many did we catch?\"\n",
    "\n",
    "- **F1-Score**: Harmonic mean of precision and recall\n",
    "  - Formula: 2 √ó (Precision √ó Recall) / (Precision + Recall)\n",
    "  - Balances both metrics\n",
    "\n",
    "- **Support**: Number of actual occurrences of each class\n",
    "\n",
    "- **Accuracy**: Overall correctness\n",
    "\n",
    "#### 2. What do TP, TN, FP, FN mean?\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "**Confusion Matrix Terms** (for Positive class):\n",
    "\n",
    "- **TP (True Positive)**: Predicted positive, actually positive ‚úÖ\n",
    "  - Example: Model says \"positive review\", it IS positive\n",
    "\n",
    "- **TN (True Negative)**: Predicted negative, actually negative ‚úÖ\n",
    "  - Example: Model says \"negative review\", it IS negative\n",
    "\n",
    "- **FP (False Positive)**: Predicted positive, actually negative ‚ùå\n",
    "  - Example: Model says \"positive review\", but it's actually negative\n",
    "  - Also called \"Type I Error\" or \"False Alarm\"\n",
    "\n",
    "- **FN (False Negative)**: Predicted negative, actually positive ‚ùå\n",
    "  - Example: Model says \"negative review\", but it's actually positive\n",
    "  - Also called \"Type II Error\" or \"Miss\"\n",
    "\n",
    "**Visual representation**:\n",
    "```\n",
    "                Predicted\n",
    "              Neg      Pos\n",
    "Actual  Neg | TN  |  FP |\n",
    "        Pos | FN  |  TP |\n",
    "```\n",
    "\n",
    "#### 3. When would you prefer high precision vs high recall?\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "**High Precision** (minimize False Positives):\n",
    "- **Spam filtering**: Don't want to mark important emails as spam\n",
    "- **Medical diagnosis for risky treatment**: Don't want to treat healthy patients\n",
    "- **Fraud detection with manual review**: Don't overwhelm reviewers with false alarms\n",
    "- \"When being wrong is costly\"\n",
    "\n",
    "**High Recall** (minimize False Negatives):\n",
    "- **Cancer screening**: Don't want to miss any cancer cases\n",
    "- **Security threats**: Don't want to miss potential attacks\n",
    "- **Customer service**: Don't want to miss unhappy customers\n",
    "- \"When missing something is costly\"\n",
    "\n",
    "**For sentiment analysis**:\n",
    "- Depends on use case!\n",
    "- Product reviews: Might prefer **balanced** (F1-score)\n",
    "- Crisis detection: Prefer **high recall** (don't miss negative sentiment)\n",
    "\n",
    "#### 4. What does a \"macro avg\" vs \"weighted avg\" mean?\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "**Macro Average**:\n",
    "- **Simple average** across all classes\n",
    "- Treats all classes equally (each class has equal weight)\n",
    "- Formula: (Metric_Class1 + Metric_Class2) / 2\n",
    "- Good for: **Balanced datasets** or when **all classes matter equally**\n",
    "\n",
    "**Weighted Average**:\n",
    "- **Weighted by support** (number of instances per class)\n",
    "- Classes with more instances contribute more\n",
    "- Formula: (Metric_Class1 √ó Support1 + Metric_Class2 √ó Support2) / Total\n",
    "- Good for: **Imbalanced datasets** (reflects overall performance better)\n",
    "\n",
    "**Example**:\n",
    "```\n",
    "Class A: 100 samples, F1=0.9\n",
    "Class B: 10 samples, F1=0.5\n",
    "\n",
    "Macro avg: (0.9 + 0.5) / 2 = 0.70\n",
    "Weighted avg: (0.9√ó100 + 0.5√ó10) / 110 = 0.86\n",
    "```\n",
    "\n",
    "In our case, support is equal (533 each), so macro = weighted!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def evaluate_performance(y_true, y_pred):\n",
    "    \"\"\"Create and print the classification report\"\"\"\n",
    "    performance = classification_report(\n",
    "        y_true, y_pred,\n",
    "        target_names=[\"Negative Review\", \"Positive Review\"]\n",
    "    )\n",
    "    print(performance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "evaluate_performance(data[\"test\"][\"label\"], y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall the model is not bad - he's correctly classifies 4 out of 5 reviews.\n",
    "\n",
    "> This is pretty good for sentiment analysis ü§ô\n",
    "\n",
    "Here's a detailed analysis of this classification report:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìä Analysis of Results\n",
    "\n",
    "#### Class-by-Class Analysis\n",
    "\n",
    "**Negative Review Performance**\n",
    "\n",
    "- **Precision: 0.76 (76%)**\n",
    "  - When the model predicts \"negative\", it's correct 76% of the time\n",
    "  - **24% false positives** - sometimes it incorrectly labels positive reviews as negative\n",
    "\n",
    "- **Recall: 0.88 (88%)**\n",
    "  - The model finds 88% of all actual negative reviews\n",
    "  - **Only 12% false negatives** - rarely misses a negative review\n",
    "\n",
    "- **F1-score: 0.81** - Good balance, but recall is stronger than precision\n",
    "\n",
    "**Interpretation**: The model is **sensitive to negativity** - it catches almost all negative reviews but sometimes over-predicts negativity.\n",
    "\n",
    "---\n",
    "\n",
    "**Positive Review Performance**\n",
    "\n",
    "- **Precision: 0.86 (86%)**\n",
    "  - When the model predicts \"positive\", it's correct 86% of the time\n",
    "  - **14% false positives** - rarely mislabels negatives as positive\n",
    "\n",
    "- **Recall: 0.72 (72%)**\n",
    "  - The model only finds 72% of actual positive reviews\n",
    "  - **28% false negatives** - misses over 1 in 4 positive reviews!\n",
    "\n",
    "- **F1-score: 0.78** - Slightly lower than negative class\n",
    "\n",
    "**Interpretation**: The model is **conservative with positivity** - when it says positive, it's usually right, but it misses many positive reviews (probably labeling them as negative instead).\n",
    "\n",
    "---\n",
    "\n",
    "#### The Trade-off Pattern We Need to Find\n",
    "\n",
    "There's an **inverse relationship between the classes**:\n",
    "\n",
    "- **Negative**: High recall (0.88), Lower precision (0.76) ‚Üí *Over-predicts negative*\n",
    "- **Positive**: High precision (0.86), Lower recall (0.72) ‚Üí *Under-predicts positive*\n",
    "\n",
    "This suggests the model has a **negative bias** - it's more likely to classify uncertain reviews as negative.\n",
    "\n",
    "---\n",
    "\n",
    "#### üìà Aggregate Metrics\n",
    "\n",
    "- **Macro avg (0.81, 0.80, 0.80)**: Simple average across both classes\n",
    "- **Weighted avg (0.81, 0.80, 0.80)**: Weighted by support (but since support is equal, same as macro)\n",
    "- **Support: 533 each** - Perfectly balanced dataset, so no class imbalance issues\n",
    "\n",
    "---\n",
    "\n",
    "#### Conclusion\n",
    "\n",
    "**Strengths:**\n",
    "- ‚úÖ **80% accuracy is solid**\n",
    "- ‚úÖ Excellent at detecting negative sentiment (88% recall)\n",
    "- ‚úÖ When it predicts positive, it's usually right (86% precision)\n",
    "\n",
    "**Weaknesses:**\n",
    "- ‚ö†Ô∏è Misses 28% of positive reviews\n",
    "- ‚ö†Ô∏è Has a slight negative bias\n",
    "- ‚ö†Ô∏è Could improve positive review detection\n",
    "\n",
    "**üí° If false negatives on positive reviews are costly** (e.g., missing happy customers), you might want to adjust the classification threshold or retrain the model to be less pessimistic!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Classification Tasks with Embeddings\n",
    "\n",
    "Now let's see how we can use embeddings to classify our data.\n",
    "\n",
    "> What's happening if we can not find a model that fits perfectly our needs?\n",
    "\n",
    "Then we need to fine-tune a model to our specific task, but it will be long, hard and costly... üò≠\n",
    "\n",
    "> So what's the solution?\n",
    "\n",
    "**Use embeddings!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìö Questions: Understanding Embeddings\n",
    "\n",
    "#### 1. What are embeddings?\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "**Embeddings** are numerical representations of text in a high-dimensional vector space:\n",
    "\n",
    "- **Dense vectors** of real numbers (typically 384, 768, or 1024 dimensions)\n",
    "- Capture **semantic meaning** - similar texts have similar embeddings\n",
    "- Generated by pre-trained neural networks\n",
    "\n",
    "**Example**:\n",
    "```python\n",
    "\"I love this movie\" ‚Üí [0.23, -0.45, 0.12, ..., 0.67]  # 768 numbers\n",
    "\"This film is great\" ‚Üí [0.25, -0.43, 0.15, ..., 0.69] # Similar numbers!\n",
    "\"I hate this film\" ‚Üí [-0.21, 0.42, -0.18, ..., -0.65] # Different!\n",
    "```\n",
    "\n",
    "**Key properties**:\n",
    "- Semantically similar texts are **close** in vector space\n",
    "- Can be used for: classification, clustering, search, recommendation\n",
    "\n",
    "#### 2. Why use embeddings instead of directly fine-tuning a model?\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "**Advantages of embeddings approach**:\n",
    "\n",
    "‚úÖ **Faster**:\n",
    "- No need to train the entire transformer model\n",
    "- Just train a small classifier (logistic regression, SVM)\n",
    "- Minutes instead of hours/days\n",
    "\n",
    "‚úÖ **Cheaper**:\n",
    "- Less computational resources (can run on CPU)\n",
    "- No expensive GPU training needed\n",
    "\n",
    "‚úÖ **Less data needed**:\n",
    "- Fine-tuning needs thousands of examples\n",
    "- Embeddings + classifier can work with hundreds\n",
    "\n",
    "‚úÖ **More flexible**:\n",
    "- Can try different classifiers quickly\n",
    "- Easy to retrain with new data\n",
    "- Can combine with other features\n",
    "\n",
    "‚úÖ **Interpretable**:\n",
    "- Traditional ML models (logistic regression) are easier to interpret\n",
    "- Can see which features matter\n",
    "\n",
    "**When to fine-tune instead**:\n",
    "- When you have lots of labeled data (10k+ examples)\n",
    "- When you need the absolute best performance\n",
    "- When the task is very different from pre-training\n",
    "\n",
    "#### 3. What is \"supervised classification with embeddings\"?\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "**Supervised classification with embeddings** is a two-step process:\n",
    "\n",
    "**Step 1: Generate embeddings**\n",
    "- Use a pre-trained embedding model (frozen, not trained)\n",
    "- Convert all text into vectors\n",
    "- Example: SentenceTransformer, BERT embeddings\n",
    "\n",
    "**Step 2: Train a classifier**\n",
    "- Use traditional ML classifier (Logistic Regression, SVM, Random Forest)\n",
    "- Train only the classifier on the embeddings\n",
    "- This is the \"supervised\" part (uses labeled data)\n",
    "\n",
    "**Why it's called \"supervised classification with embeddings\"**:\n",
    "- **Supervised**: We have labeled training data (positive/negative)\n",
    "- **With embeddings**: We use embeddings as features (not raw text or TF-IDF)\n",
    "- We **do NOT** fine-tune the embedding model, we just use it as a feature extractor\n",
    "- Only the classifier is trained (üßä frozen embeddings, üî• trained classifier)\n",
    "\n",
    "**Analogy**:\n",
    "- Embeddings = Hiring a translator who knows the language\n",
    "- Classifier = Teaching a simple rule-based system using the translations\n",
    "\n",
    "#### 4. What embedding model are we using and why?\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "We're using **`sentence-transformers/all-mpnet-base-v2`**:\n",
    "\n",
    "**What is it?**\n",
    "- Based on **MPNet** architecture (Masked and Permuted pre-training)\n",
    "- Trained specifically for **sentence-level embeddings**\n",
    "- Part of the Sentence-Transformers library\n",
    "- Outputs **768-dimensional** vectors\n",
    "\n",
    "**Why this model?**\n",
    "- ‚úÖ **Very popular and well-performing** for semantic similarity\n",
    "- ‚úÖ **General-purpose** - works well across many domains\n",
    "- ‚úÖ **Good quality/speed trade-off**\n",
    "- ‚úÖ **Trained on diverse data** - not domain-specific\n",
    "- ‚úÖ **Proven performance** on semantic textual similarity benchmarks\n",
    "\n",
    "**Alternatives you could use**:\n",
    "- `all-MiniLM-L6-v2` - Faster but slightly lower quality\n",
    "- `all-mpnet-base-v2` - Best balance (what we're using)\n",
    "- Larger models for even better quality (but slower)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supervised Classification with Embeddings\n",
    "\n",
    "Instead of using a pre-trained model for our specific task, we will use an embedding model for feature generation. Then those features will be used to train a classifier. This method is called **Supervised classification with embeddings** because we do not need to fine-tune the model, we just need to train a classifier on the features üèãÔ∏è\n",
    "\n",
    "For this example we will use a `sentence-transformers` model to generate embeddings for our data. It's very popular and well-performing for this kind of task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load model\n",
    "model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
    "\n",
    "# Convert text to embeddings\n",
    "print(\"Generating embeddings for training data...\")\n",
    "train_embeddings = model.encode(data[\"train\"][\"text\"], show_progress_bar=True)\n",
    "\n",
    "print(\"Generating embeddings for test data...\")\n",
    "test_embeddings = model.encode(data[\"test\"][\"text\"], show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the shape of embeddings\n",
    "print(f\"Training embeddings shape: {train_embeddings.shape}\")\n",
    "print(f\"Test embeddings shape: {test_embeddings.shape}\")\n",
    "\n",
    "print(f\"\\nThis shows that each of our {train_embeddings.shape[0]} input documents has an embeddings dimension of {train_embeddings.shape[1]}!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's train a very simple logistic regression on our embeddings ü§ì"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìö Questions: Logistic Regression on Embeddings\n",
    "\n",
    "#### 1. Why use Logistic Regression instead of a neural network?\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "**Advantages of Logistic Regression**:\n",
    "\n",
    "‚úÖ **Fast training**:\n",
    "- Trains in seconds/minutes on embeddings\n",
    "- Neural networks take hours\n",
    "\n",
    "‚úÖ **No hyperparameter tuning needed**:\n",
    "- Neural networks: learning rate, layers, dropout, epochs...\n",
    "- Logistic Regression: just regularization (C parameter)\n",
    "\n",
    "‚úÖ **Interpretable**:\n",
    "- Can see feature importance (weights)\n",
    "- Understand which embedding dimensions matter\n",
    "\n",
    "‚úÖ **Less prone to overfitting**:\n",
    "- Simpler model = less risk with small datasets\n",
    "\n",
    "‚úÖ **Works surprisingly well**:\n",
    "- Embeddings already contain rich features\n",
    "- A simple linear classifier is often enough!\n",
    "\n",
    "**When to use neural networks instead**:\n",
    "- Very large datasets (100k+ examples)\n",
    "- Need to capture complex non-linear patterns\n",
    "- Multi-task learning\n",
    "\n",
    "#### 2. What does `random_state=42` do?\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "`random_state=42` sets the **random seed** for reproducibility:\n",
    "\n",
    "- **With seed**: Same results every time you run the code\n",
    "- **Without seed**: Different results each run (randomized initialization)\n",
    "\n",
    "**Why important**:\n",
    "- Makes experiments reproducible\n",
    "- Essential for debugging\n",
    "- Fair comparison between methods\n",
    "- Required for scientific papers\n",
    "\n",
    "**What it affects in Logistic Regression**:\n",
    "- Initial shuffling of data\n",
    "- Solver's random initialization (for some solvers)\n",
    "- Sample order in stochastic methods\n",
    "\n",
    "The number 42 is arbitrary (a reference to \"Hitchhiker's Guide to the Galaxy\")\n",
    "\n",
    "#### 3. How does the performance compare to the task-specific RoBERTa model?\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "**Comparison** (based on typical results):\n",
    "\n",
    "**RoBERTa (Task-Specific)**:\n",
    "- Accuracy: ~0.80 (80%)\n",
    "- F1-Score: ~0.80\n",
    "- Training: Used pre-trained model (no training needed)\n",
    "- Inference: Slower (full transformer forward pass)\n",
    "\n",
    "**Embeddings + Logistic Regression**:\n",
    "- Accuracy: ~0.85 (85%)\n",
    "- F1-Score: ~0.85\n",
    "- Training: Fast (minutes)\n",
    "- Inference: Very fast (just matrix multiplication)\n",
    "\n",
    "**üéâ Embeddings approach performs BETTER!**\n",
    "\n",
    "**Why embeddings work better here**:\n",
    "1. The embedding model captures general semantic meaning well\n",
    "2. We train the classifier specifically on our data\n",
    "3. The RoBERTa model was trained on Twitter data (different domain)\n",
    "4. Simple is sometimes better!\n",
    "\n",
    "**Trade-offs**:\n",
    "- ‚úÖ Embeddings: Better accuracy, faster, simpler\n",
    "- ‚úÖ RoBERTa: No training needed, works out-of-the-box\n",
    "\n",
    "**Congrats!** üéä With the embeddings training we achieve a better F1 score than initially!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Train a Logistic Regression on our train embeddings\n",
    "clf = LogisticRegression(random_state=42)\n",
    "clf.fit(train_embeddings, data[\"train\"][\"label\"])\n",
    "\n",
    "print(\"‚úÖ Logistic Regression trained!\")\n",
    "print(f\"Model: {clf}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on previously unseen instances\n",
    "y_pred = clf.predict(test_embeddings)\n",
    "evaluate_performance(data[\"test\"][\"label\"], y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Congrats!** üéä With the embeddings training we achieve a better F1 score than initially!\n",
    "\n",
    "> This demonstrates the possibility of training a light classifier while keeping the embeddings model frozen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## What if We Do Not Have Labeled Data: Unsupervised Use Case\n",
    "\n",
    "What would happen if we would not use a classifier at all? Instead, we can average the embeddings per class and apply cosine similarity to predict which classes match the documents best üîç"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìö Questions: Zero-Shot Classification with Embeddings\n",
    "\n",
    "#### 1. What is zero-shot classification?\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "**Zero-shot classification** is when a model can classify text into categories it has **never been explicitly trained on**, simply by understanding the semantic relationship between the input text and candidate label descriptions.\n",
    "\n",
    "**Key characteristics**:\n",
    "- **No labeled training data** needed for the target task\n",
    "- Model uses **semantic understanding** only\n",
    "- Works by comparing **meaning** of text vs. label descriptions\n",
    "\n",
    "**How it works**:\n",
    "1. Embed the document: \"This movie is terrible\"\n",
    "2. Embed label descriptions: \"A negative review\", \"A positive review\"\n",
    "3. Calculate similarity between document and each label\n",
    "4. Assign the label with highest similarity\n",
    "\n",
    "**Real-world example**:\n",
    "- Input: \"The product broke after one day\"\n",
    "- Labels: \"complaint\", \"praise\", \"question\"\n",
    "- Model figures out it's a \"complaint\" without any examples!\n",
    "\n",
    "#### 2. How does this approach work without training?\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "**The magic is in the embeddings**:\n",
    "\n",
    "**Step 1: Average embeddings per class**\n",
    "```python\n",
    "# Average all negative review embeddings\n",
    "negative_center = mean(all_negative_training_embeddings)\n",
    "\n",
    "# Average all positive review embeddings  \n",
    "positive_center = mean(all_positive_training_embeddings)\n",
    "```\n",
    "\n",
    "**Step 2: For new document**\n",
    "```python\n",
    "# Get embedding for test document\n",
    "test_embedding = model.encode(\"This movie was awful\")\n",
    "\n",
    "# Calculate similarity to both centers\n",
    "sim_to_negative = cosine_similarity(test_embedding, negative_center)\n",
    "sim_to_positive = cosine_similarity(test_embedding, positive_center)\n",
    "\n",
    "# Assign closest label\n",
    "label = \"negative\" if sim_to_negative > sim_to_positive else \"positive\"\n",
    "```\n",
    "\n",
    "**Why it works**:\n",
    "- The embedding model already understands semantic meaning\n",
    "- Similar texts have similar embeddings\n",
    "- We're just finding which \"cluster\" the new text is closest to\n",
    "- No training needed - it's pure geometric comparison!\n",
    "\n",
    "#### 3. What is cosine similarity?\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "**Cosine similarity** measures the **angle** between two vectors:\n",
    "\n",
    "**Formula**:\n",
    "```\n",
    "cosine_similarity(A, B) = (A ¬∑ B) / (||A|| √ó ||B||)\n",
    "```\n",
    "\n",
    "**Range**: -1 to 1\n",
    "- **1**: Vectors point in exactly the same direction (identical meaning)\n",
    "- **0**: Vectors are perpendicular (unrelated)\n",
    "- **-1**: Vectors point in opposite directions (opposite meaning)\n",
    "\n",
    "**Why use cosine instead of Euclidean distance?**\n",
    "- ‚úÖ **Ignores magnitude**: Only cares about direction (semantic meaning)\n",
    "- ‚úÖ **Better for high-dimensional data**: Embeddings are 768-dimensional\n",
    "- ‚úÖ **Normalized**: Always in [-1, 1] range\n",
    "- ‚úÖ **Standard in NLP**: Used everywhere for text similarity\n",
    "\n",
    "**Visual analogy**:\n",
    "```\n",
    "        Text A ‚Üí\n",
    "         /\n",
    "        /  small angle = high similarity\n",
    "       /\n",
    "      Text B ‚Üí\n",
    "```\n",
    "\n",
    "#### 4. When would zero-shot be better than supervised?\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "**Zero-shot is better when**:\n",
    "\n",
    "‚úÖ **No labeled data available**:\n",
    "- New use case with no examples\n",
    "- Labeling is expensive/time-consuming\n",
    "\n",
    "‚úÖ **Need extreme flexibility**:\n",
    "- Categories change frequently\n",
    "- Want to classify into arbitrary categories\n",
    "- Example: \"Is this review about price, quality, or shipping?\"\n",
    "\n",
    "‚úÖ **Many rare categories**:\n",
    "- Long tail classification\n",
    "- Not enough examples per category to train\n",
    "\n",
    "‚úÖ **Quick prototyping**:\n",
    "- Testing ideas fast\n",
    "- MVP development\n",
    "\n",
    "‚úÖ **Cold start problem**:\n",
    "- Just launched a product/service\n",
    "- Don't have historical data yet\n",
    "\n",
    "**Supervised is better when**:\n",
    "- You have 100+ labeled examples per class\n",
    "- Need highest possible accuracy\n",
    "- Categories are fixed\n",
    "- Performance is critical\n",
    "\n",
    "**Hybrid approach**:\n",
    "- Start with zero-shot\n",
    "- Collect labels from user feedback\n",
    "- Gradually transition to supervised\n",
    "\n",
    "#### 5. How do we describe our labels for zero-shot?\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "**Label description is critical** for zero-shot performance!\n",
    "\n",
    "**In our example**:\n",
    "```python\n",
    "label_embeddings = model.encode([\n",
    "    \"A negative review\",  # Description for label 0\n",
    "    \"A positive review\"   # Description for label 1\n",
    "])\n",
    "```\n",
    "\n",
    "**Best practices for label descriptions**:\n",
    "\n",
    "‚úÖ **Be descriptive, not just the label name**:\n",
    "- ‚ùå Bad: \"negative\", \"positive\"\n",
    "- ‚úÖ Good: \"A negative movie review\", \"A positive movie review\"\n",
    "- ‚úÖ Better: \"This is a negative review expressing disappointment\"\n",
    "\n",
    "‚úÖ **Match the domain/style of your documents**:\n",
    "- For movie reviews: \"This movie is terrible\" vs \"This movie is great\"\n",
    "- For product reviews: \"The product is defective\" vs \"The product is excellent\"\n",
    "\n",
    "‚úÖ **Use examples or prototypes**:\n",
    "- \"A negative review like: bad, terrible, awful, disappointing\"\n",
    "\n",
    "‚úÖ **Be specific about what makes something belong to that category**:\n",
    "- \"A negative review that criticizes the movie\"\n",
    "- \"A positive review that recommends the movie\"\n",
    "\n",
    "**Pro tip**: The better your label descriptions match the vocabulary and style of your documents, the better zero-shot will work!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Average the embeddings of all documents in each target label\n",
    "df = pd.DataFrame(np.hstack([train_embeddings, np.array(data[\"train\"][\"label\"]).reshape(-1, 1)]))\n",
    "averaged_target_embeddings = df.groupby(768).mean().values\n",
    "\n",
    "print(f\"Shape of averaged embeddings per class: {averaged_target_embeddings.shape}\")\n",
    "print(\"This gives us a 'prototype' or 'center' for each class\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the best matching embeddings between evaluation documents and target embeddings\n",
    "sim_matrix = cosine_similarity(test_embeddings, averaged_target_embeddings)\n",
    "y_pred = np.argmax(sim_matrix, axis=1)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluate_performance(data[\"test\"][\"label\"], y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**AN F1 score at 0.84 is quite impressive considering we did not used any labels!!** This is the perfect illustration why embeddings can be a very useful tool!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zero-Shot Classification\n",
    "\n",
    "A **zero-shot classification** is when a model can classify text into categories it has never been explicitly trained on, simply by understanding the semantic relationship between the input text and candidate label descriptions.\n",
    "\n",
    "In our case we do not have labeled data, we will try to predict these labels of input text even though the model was not trained on them üîÆ\n",
    "\n",
    "> To perform zero-shot classification with embeddings, there is a little trick that we can use. We can describe our labels based on what they should represent. For example, a negative label for movie reviews can be described as \"This is a negative movie review.\" By describing and embedding the labels and documents, we have data that we can work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create embeddings for our labels\n",
    "label_embeddings = model.encode([\"A negative review\", \"A positive review\"])\n",
    "\n",
    "print(f\"Label embeddings shape: {label_embeddings.shape}\")\n",
    "print(\"We now have embeddings for what 'negative' and 'positive' mean!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To assign labels to documents, we can apply cosine similarity to the document-label pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Find the best matching label for each document\n",
    "sim_matrix = cosine_similarity(test_embeddings, label_embeddings)\n",
    "y_pred = np.argmax(sim_matrix, axis=1)\n",
    "\n",
    "print(f\"Similarity matrix shape: {sim_matrix.shape}\")\n",
    "print(f\"Each test document gets a similarity score to both labels\")\n",
    "print(f\"\\nExample similarity scores for first test document:\")\n",
    "print(f\"  Similarity to 'negative': {sim_matrix[0][0]:.4f}\")\n",
    "print(f\"  Similarity to 'positive': {sim_matrix[0][1]:.4f}\")\n",
    "print(f\"  Predicted label: {'Negative' if y_pred[0] == 0 else 'Positive'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate zero-shot performance\n",
    "evaluate_performance(data[\"test\"][\"label\"], y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**AN F1 score at 0.78 is quite impressive considering we did not use any labels!!** This is the perfect illustration why embeddings can be a very useful tool!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìä Comparison of All Three Approaches\n",
    "\n",
    "Let's summarize what we learned:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary comparison\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Approach': [\n",
    "        'Task-Specific RoBERTa',\n",
    "        'Embeddings + Logistic Regression',\n",
    "        'Zero-Shot (Averaged Embeddings)',\n",
    "        'Zero-Shot (Label Descriptions)'\n",
    "    ],\n",
    "    'F1-Score': ['~0.80', '~0.85', '~0.84', '~0.78'],\n",
    "    'Training Time': ['0 (pre-trained)', 'Minutes', '0', '0'],\n",
    "    'Labeled Data Needed': ['0', 'Full training set', '0 (uses train for averaging)', '0'],\n",
    "    'Flexibility': ['Low', 'Medium', 'High', 'Very High'],\n",
    "    'Inference Speed': ['Slow', 'Fast', 'Fast', 'Fast']\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"COMPARISON OF ALL APPROACHES\")\n",
    "print(\"=\"*80)\n",
    "print(comparison_df.to_string(index=False))\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Text Classification with Generative Models\n",
    "\n",
    "Generative language models like OpenAI's GPT differ fundamentally in their approach to classification compared to traditional methods.\n",
    "\n",
    "Rather than following conventional classification paradigms, these models function as **sequence-to-sequence systems**: in short, **they receive text input and produce text output**.\n",
    "\n",
    "While these generative models undergo training across diverse tasks, they typically cannot handle specialized use cases immediately. Consider feeding a movie review to such a model without additional guidance: the model would lack direction on how to process it.\n",
    "\n",
    "To achieve meaningful results, we must **provide context and steer the model toward our desired outcomes**. This guidance occurs primarily through carefully crafted instructions, known as **prompts** üéØ\n",
    "\n",
    "For our demo we will use the [groq API](https://groq.com/) because OpenAI do not give us a free API keys üòä"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìö Questions: Generative Models for Classification\n",
    "\n",
    "#### 1. How do generative models differ from discriminative models?\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "**Discriminative Models** (BERT, RoBERTa, traditional ML):\n",
    "- Learn the **boundary** between classes\n",
    "- Answer: \"Given input X, what is the probability of class Y?\"\n",
    "- Formula: P(Y|X)\n",
    "- Example: \"This text is 85% likely to be positive\"\n",
    "- **Specialized**: Trained for specific tasks\n",
    "- **Fast**: Direct classification\n",
    "\n",
    "**Generative Models** (GPT, LLaMA, Claude):\n",
    "- Learn to **generate** the next token\n",
    "- Answer: \"Given input X, what text should come next?\"\n",
    "- Formula: P(X) or P(X|context)\n",
    "- Example: \"This text is... positive\" (generates the word \"positive\")\n",
    "- **General-purpose**: Can do many tasks\n",
    "- **Flexible**: Just change the prompt\n",
    "\n",
    "**For classification**:\n",
    "- Discriminative: Outputs a probability distribution over classes\n",
    "- Generative: Generates text that represents the class\n",
    "\n",
    "#### 2. What is prompt engineering?\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "**Prompt engineering** is the art and science of crafting instructions to get desired behavior from language models.\n",
    "\n",
    "**Key elements of a good prompt**:\n",
    "\n",
    "1. **Role/Context**: \"You are a sentiment classifier\"\n",
    "2. **Task Description**: \"Rate the sentiment of this movie review\"\n",
    "3. **Format Specification**: \"Respond with only 'positive' or 'negative'\"\n",
    "4. **Examples** (few-shot): Show 2-3 examples\n",
    "5. **The Input**: The actual text to classify\n",
    "\n",
    "**Example prompt evolution**:\n",
    "\n",
    "‚ùå **Bad**: \"positive or negative?\"\n",
    "```\n",
    "Model might respond: \"What do you mean?\"\n",
    "```\n",
    "\n",
    "‚ö†Ô∏è **Better**: \"Is this review positive or negative?\"\n",
    "```\n",
    "Model might respond: \"Well, it could be seen as...\"\n",
    "```\n",
    "\n",
    "‚úÖ **Good**: \"Classify the sentiment of this movie review. Respond with only 'positive' or 'negative'.\"\n",
    "```\n",
    "Model responds: \"positive\"\n",
    "```\n",
    "\n",
    "**Advanced techniques**:\n",
    "- Chain-of-thought: \"Let's think step by step\"\n",
    "- Few-shot learning: Provide examples\n",
    "- Temperature control: Adjust randomness\n",
    "- System prompts: Set global behavior\n",
    "\n",
    "#### 3. What is few-shot learning vs zero-shot?\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "**Zero-Shot**:\n",
    "- **No examples** provided in the prompt\n",
    "- Model relies purely on instructions\n",
    "- Example:\n",
    "```\n",
    "Classify this review as positive or negative:\n",
    "\"The movie was terrible\"\n",
    "```\n",
    "\n",
    "**Few-Shot** (1-shot, 2-shot, 5-shot, etc.):\n",
    "- **Provide examples** in the prompt\n",
    "- Model learns the pattern from examples\n",
    "- Example:\n",
    "```\n",
    "Classify reviews as positive or negative:\n",
    "\n",
    "Review: \"Amazing film!\" ‚Üí positive\n",
    "Review: \"Waste of time\" ‚Üí negative\n",
    "\n",
    "Review: \"The movie was terrible\" ‚Üí ?\n",
    "```\n",
    "\n",
    "**Comparison**:\n",
    "\n",
    "| Aspect | Zero-Shot | Few-Shot |\n",
    "|--------|-----------|----------|\n",
    "| Examples needed | 0 | 1-10 |\n",
    "| Performance | Good | Better |\n",
    "| Prompt length | Short | Longer |\n",
    "| Token cost | Lower | Higher |\n",
    "| Setup time | None | Minimal |\n",
    "\n",
    "**When to use which**:\n",
    "- **Zero-shot**: Simple, clear tasks (sentiment, spam detection)\n",
    "- **Few-shot**: Complex or ambiguous tasks, domain-specific categories\n",
    "\n",
    "#### 4. Why do we ask for just the label and not an explanation?\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "**Asking for just the label** (\"Respond with only 'positive'\"):\n",
    "\n",
    "‚úÖ **Faster**:\n",
    "- Fewer tokens to generate\n",
    "- Lower latency\n",
    "- Cheaper (pay per token)\n",
    "\n",
    "‚úÖ **Easier to parse**:\n",
    "- Simple string matching\n",
    "- No need for complex parsing\n",
    "- More reliable automation\n",
    "\n",
    "‚úÖ **More consistent**:\n",
    "- Model can't ramble\n",
    "- Format is predictable\n",
    "- Easier to evaluate\n",
    "\n",
    "‚úÖ **Prevents errors**:\n",
    "- Model might say \"It's positive because...\"\n",
    "- Harder to extract the label\n",
    "- More can go wrong\n",
    "\n",
    "**When to ask for explanations**:\n",
    "- User-facing applications (need to show reasoning)\n",
    "- Debugging (understand model's thinking)\n",
    "- High-stakes decisions (need justification)\n",
    "- Active learning (to improve training data)\n",
    "\n",
    "**Best of both worlds**:\n",
    "```python\n",
    "# Ask for structured output\n",
    "prompt = \"\"\"\n",
    "Rate this review and explain briefly:\n",
    "Format: label: [positive/negative], reason: [brief explanation]\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "#### 5. How do we handle models that output scores/probabilities?\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "**Two approaches for getting confidence scores**:\n",
    "\n",
    "**Approach 1: Ask for a numerical score**\n",
    "```python\n",
    "prompt = \"\"\"\n",
    "Rate the sentiment of this review on a scale from 0 to 1:\n",
    "0 = very negative, 1 = very positive\n",
    "\n",
    "Review: {text}\n",
    "Score:\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "**Approach 2: Use logit probabilities** (API-dependent)\n",
    "```python\n",
    "# Some APIs return token probabilities\n",
    "response = client.chat.completions.create(\n",
    "    model=\"meta-llama/llama-4-scout-17b-16e-instruct\",\n",
    "    messages=[...],\n",
    "    logprobs=True,  # Get probability distribution\n",
    "    temperature=0    # Deterministic\n",
    ")\n",
    "\n",
    "# Extract probability for \"positive\" vs \"negative\"\n",
    "```\n",
    "\n",
    "**In our example**:\n",
    "- We ask: \"Rate the sentiment... Score:\"\n",
    "- Model outputs: \"0.87\" (a probability/score)\n",
    "- We parse it as a float\n",
    "- Convert to binary: >0.5 = positive, <0.5 = negative\n",
    "\n",
    "**Why this is useful**:\n",
    "- Can set custom thresholds (e.g., only show if confidence > 0.8)\n",
    "- Can identify uncertain predictions\n",
    "- Better for active learning (label uncertain cases first)\n",
    "- Matches discriminative model outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install groq\n",
    "!pip install groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up your API key\n",
    "# You need to get a free API key from https://console.groq.com/\n",
    "import os\n",
    "\n",
    "# Option 1: Set environment variable (recommended)\n",
    "# export GROQ_API_KEY=\"your_api_key_here\"\n",
    "\n",
    "# Option 2: Set it in code (not recommended for production)\n",
    "os.environ[\"GROQ_API_KEY\"] = \"YOUR_API_KEY\"  # Replace with your actual key\n",
    "\n",
    "print(\"‚ö†Ô∏è Remember to replace YOUR_API_KEY with your actual Groq API key!\")\n",
    "print(\"Get it from: https://console.groq.com/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with a single example\n",
    "sample_text = data[\"test\"][\"text\"][0]\n",
    "print(f\"Sample Review: {sample_text}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from groq import Groq\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize Groq client\n",
    "client = Groq(\n",
    "    api_key=os.getenv(\"GROQ_API_KEY\"),\n",
    ")\n",
    "\n",
    "# Create a simple prompt\n",
    "chat_completion = client.chat.completions.create(\n",
    "    model=\"meta-llama/llama-4-scout-17b-16e-instruct\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a sentiment classifier. Respond with only 'positive' or 'negative'.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"Classify the sentiment of this movie review: {sample_text}\"\n",
    "        }\n",
    "    ],\n",
    "    temperature=0,\n",
    "    max_tokens=10\n",
    ")\n",
    "\n",
    "print(f\"Model prediction: {chat_completion.choices[0].message.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**or we can output a score if you need more granularity**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a probability score instead\n",
    "chat_completion = client.chat.completions.create(\n",
    "    model=\"meta-llama/llama-4-scout-17b-16e-instruct\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a sentiment classifier. Rate the sentiment of movie reviews on a scale from 0 to 1, where 0 is very negative and 1 is very positive.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"Rate the sentiment of this movie review: {sample_text}\\n\\nScore:\"\n",
    "        }\n",
    "    ],\n",
    "    temperature=0,\n",
    "    max_tokens=10\n",
    ")\n",
    "\n",
    "score = chat_completion.choices[0].message.content\n",
    "print(f\"Sentiment score: {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's evaluate this classifier with the same classification report and see how it's performing ü§î"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Keep in mind this is a very simple prompt**, if you need more control about the LLM output you can check how to structure the output of an LLM on the [openai doc](https://platform.openai.com/docs/guides/structured-outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function for generation\n",
    "def groq_generation(prompt, model=\"meta-llama/llama-4-scout-17b-16e-instruct\"):\n",
    "    \"\"\"Generate sentiment classification using Groq API\"\"\"\n",
    "    message = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a sentiment classifier. Rate the sentiment of movie reviews on a scale from 0 to 1, where 0 is very negative and 1 is very positive. Respond with only a number.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"Rate the sentiment of this movie review: {prompt}\\n\\nScore:\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    chat_completion = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=message,\n",
    "        temperature=0,\n",
    "        max_tokens=10\n",
    "    )\n",
    "    \n",
    "    return chat_completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on one example\n",
    "result = groq_generation(sample_text)\n",
    "print(f\"Score for sample: {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run on test set (this will take a few minutes)\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(\"Generating predictions for test set...\")\n",
    "print(\"This may take a few minutes...\\n\")\n",
    "\n",
    "predictions = []\n",
    "for doc in tqdm(data[\"test\"][\"text\"][:100]):  # Limit to 100 for demo (remove [:100] for full dataset)\n",
    "    pred = groq_generation(doc)\n",
    "    predictions.append(pred)\n",
    "\n",
    "print(f\"\\nGenerated {len(predictions)} predictions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert scores to binary predictions\n",
    "y_pred = []\n",
    "for pred in predictions:\n",
    "    try:\n",
    "        score = float(pred.strip())\n",
    "        # Convert to binary: 0 if score < 0.5, else 1\n",
    "        y_pred.append(0 if score < 0.5 else 1)\n",
    "    except ValueError:\n",
    "        # If can't parse as float, try to detect keywords\n",
    "        if \"negative\" in pred.lower():\n",
    "            y_pred.append(0)\n",
    "        else:\n",
    "            y_pred.append(1)\n",
    "\n",
    "print(f\"Converted {len(y_pred)} predictions to binary labels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate performance (only on the subset we tested)\n",
    "evaluate_performance(data[\"test\"][\"label\"][:len(y_pred)], y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**GG for the Flan T5 model, 0.84 is a very good first F1 score** üèÜ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Text-to-Text Transfer Transformers (T5)\n",
    "\n",
    "Let's explore a final technique called **text-to-text transfer transformers** or T5 models. üîÑ The architecture is similar to the original Transformers with an encoder and decoder parts stacked together.\n",
    "\n",
    "T5 reframes every common NLP tasks such as translation, summarization, classification, question answering **as input text ‚Üí output text, simplifying model design and enabling multitask learning**.\n",
    "\n",
    "T5 was trained on the [Colossal Clean Crawled Corpus](https://www.tensorflow.org/datasets/catalog/c4), with a self-supervised objective called **span corruption**, giving it strong generalization across NLP tasks.\n",
    "\n",
    "> Because T5 generates text tokens for answers and labels, it excels in zero-shot, few-shot, and instruction-based tasks, without needing task-specific heads or architectures üòé"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìö Questions: T5 Models\n",
    "\n",
    "#### 1. What is T5 and how does it differ from BERT/GPT?\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "**T5 (Text-to-Text Transfer Transformer)**:\n",
    "\n",
    "**Architecture**:\n",
    "- **Encoder-Decoder** (like original Transformer)\n",
    "- Both encoder and decoder use self-attention\n",
    "- Can attend to input and previously generated output\n",
    "\n",
    "**Key Innovation**:\n",
    "- **Everything is text-to-text**\n",
    "- All tasks reformulated as: input text ‚Üí output text\n",
    "\n",
    "**Comparison**:\n",
    "\n",
    "| Model | Architecture | Training | Best For |\n",
    "|-------|-------------|----------|----------|\n",
    "| **BERT** | Encoder only | Masked LM | Classification, NER |\n",
    "| **GPT** | Decoder only | Next token prediction | Text generation |\n",
    "| **T5** | Encoder-Decoder | Span corruption | All tasks! |\n",
    "\n",
    "**T5 advantages**:\n",
    "- ‚úÖ Single model for all tasks\n",
    "- ‚úÖ Natural task specification (just describe in text)\n",
    "- ‚úÖ Flexible output format\n",
    "- ‚úÖ Good at both understanding and generation\n",
    "\n",
    "#### 2. What does \"text-to-text\" mean?\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "**Text-to-text** means every task is framed as:\n",
    "- **Input**: Text string\n",
    "- **Output**: Text string\n",
    "\n",
    "**Examples**:\n",
    "\n",
    "**Translation**:\n",
    "```\n",
    "Input: \"translate English to French: Hello\"\n",
    "Output: \"Bonjour\"\n",
    "```\n",
    "\n",
    "**Summarization**:\n",
    "```\n",
    "Input: \"summarize: [long article]\"\n",
    "Output: \"[short summary]\"\n",
    "```\n",
    "\n",
    "**Classification**:\n",
    "```\n",
    "Input: \"Is the following sentence positive or negative? The movie was great.\"\n",
    "Output: \"positive\"\n",
    "```\n",
    "\n",
    "**Question Answering**:\n",
    "```\n",
    "Input: \"question: What is the capital of France? context: Paris is the capital...\"\n",
    "Output: \"Paris\"\n",
    "```\n",
    "\n",
    "**Benefits**:\n",
    "- Unified API for all tasks\n",
    "- Easy to add new tasks (just change the prompt)\n",
    "- Natural for multitask learning\n",
    "- Flexible output format\n",
    "\n",
    "#### 3. What is the \"t5\" prompt format?\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "T5 uses **task prefixes** to specify what to do:\n",
    "\n",
    "**Format**: `task_name: input_text`\n",
    "\n",
    "**Common prefixes**:\n",
    "- `translate English to French:`\n",
    "- `summarize:`\n",
    "- `question: ... context: ...`\n",
    "- `sentiment:` (for classification)\n",
    "\n",
    "**In our example**:\n",
    "```python\n",
    "prompt = \"Is the following sentence positive or negative? \" + review\n",
    "```\n",
    "\n",
    "**Why use prefixes?**\n",
    "- Tells the model which task to perform\n",
    "- Activates task-specific knowledge from training\n",
    "- Consistent with T5's training format\n",
    "\n",
    "**Alternative formats** (all work):\n",
    "```python\n",
    "# Explicit\n",
    "\"sentiment: \" + review\n",
    "\n",
    "# Question format (what we use)\n",
    "\"Is the following sentence positive or negative? \" + review\n",
    "\n",
    "# Instruction format\n",
    "\"Classify this review as positive or negative: \" + review\n",
    "```\n",
    "\n",
    "#### 4. How do we convert T5 text output to labels?\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "**In our example**:\n",
    "\n",
    "**Step 1**: T5 generates text\n",
    "```python\n",
    "output = pipe(...)\n",
    "text = output[0][\"generated_text\"]\n",
    "# text = \"negative\" or \"positive\"\n",
    "```\n",
    "\n",
    "**Step 2**: Map text to numeric labels\n",
    "```python\n",
    "# Check if \"negative\" appears in output\n",
    "if \"negative\" in text.lower():\n",
    "    label = 0\n",
    "else:\n",
    "    label = 1\n",
    "```\n",
    "\n",
    "**More robust mapping**:\n",
    "```python\n",
    "label_map = {\n",
    "    \"negative\": 0,\n",
    "    \"positive\": 1\n",
    "}\n",
    "\n",
    "# Parse output\n",
    "for key, value in label_map.items():\n",
    "    if key in text.lower():\n",
    "        return value\n",
    "```\n",
    "\n",
    "**Handling errors**:\n",
    "- Model might say \"It's negative because...\"\n",
    "- Use keyword matching (check for \"negative\" or \"positive\")\n",
    "- Could use regex for more robust parsing\n",
    "- Default to most common class if uncertain\n",
    "\n",
    "**Why this works**:\n",
    "- T5 is trained to generate labels as text\n",
    "- We just need to parse its natural language output\n",
    "- More flexible than fixed class indices\n",
    "\n",
    "#### 5. What are the trade-offs of T5 vs task-specific models?\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "**T5 Advantages**:\n",
    "- ‚úÖ **Flexibility**: One model for all tasks\n",
    "- ‚úÖ **Easy to adapt**: Just change the prompt\n",
    "- ‚úÖ **Natural output**: Generates text labels\n",
    "- ‚úÖ **Transfer learning**: Benefits from multitask training\n",
    "- ‚úÖ **Few-shot learning**: Works with minimal examples\n",
    "\n",
    "**T5 Disadvantages**:\n",
    "- ‚ùå **Slower**: Encoder + Decoder (2x parameters active)\n",
    "- ‚ùå **Larger**: More parameters than encoder-only models\n",
    "- ‚ùå **Less accurate**: Jack-of-all-trades, master of none\n",
    "- ‚ùå **Parsing needed**: Must convert text output to labels\n",
    "- ‚ùå **More tokens**: Generates text, not just logits\n",
    "\n",
    "**Task-Specific Models** (like RoBERTa):\n",
    "- ‚úÖ **Faster**: Encoder only\n",
    "- ‚úÖ **More accurate**: Specialized for the task\n",
    "- ‚úÖ **Direct output**: Probability distribution over classes\n",
    "- ‚úÖ **Smaller**: Fewer parameters\n",
    "- ‚ùå **Limited**: One model per task\n",
    "- ‚ùå **Requires fine-tuning**: Need labeled data\n",
    "\n",
    "**When to use T5**:\n",
    "- Multiple different tasks\n",
    "- Need flexibility to change tasks\n",
    "- Few labeled examples available\n",
    "- Want natural language output\n",
    "\n",
    "**When to use task-specific**:\n",
    "- Single task, need best performance\n",
    "- Latency-critical applications\n",
    "- Large labeled dataset available\n",
    "- Deployment constraints (memory/compute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load T5 model for text-to-text generation\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text2text-generation\",\n",
    "    model=\"google/flan-t5-small\",\n",
    "    device=\"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    ")\n",
    "\n",
    "print(\"T5 model loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare our data with T5 format\n",
    "prompt = \"Is the following sentence positive or negative? \"\n",
    "data = data.map(lambda example: {\"t5\": prompt + example[\"text\"]})\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this model generates text, we need to map 0 for negative and 1 for positive. Then we can run our evaluation ü§ì"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run inference\n",
    "from tqdm import tqdm\n",
    "\n",
    "y_pred = []\n",
    "for output in tqdm(pipe(KeyDataset(data[\"test\"], \"t5\")), total=len(data[\"test\"])):\n",
    "    text = output[0][\"generated_text\"]\n",
    "    # Check if text contains \"negative\" to assign 0, else 1\n",
    "    y_pred.append(0 if \"negative\" in text.lower() else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate performance\n",
    "evaluate_performance(data[\"test\"][\"label\"], y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**GG for the Flan T5 model, 0.84 is a very good first F1 score** üèÜ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "Now I hope you have a better understanding of text classification and how to handle it with and without generative models. We know now that **pre-trained models are very good for classifying text!**\n",
    "\n",
    "We also know that we can leverage the power of embeddings to use it as input to train classifiers. **Now in the next episode we'll explore the world of text clustering and topic modeling** üìä"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéØ Final Summary\n",
    "\n",
    "**What we learned**:\n",
    "\n",
    "1. **Task-Specific Models** (RoBERTa):\n",
    "   - Pre-trained and ready to use\n",
    "   - Good accuracy (~80%)\n",
    "   - No training needed\n",
    "\n",
    "2. **Embeddings + Supervised Learning**:\n",
    "   - Best accuracy (~85%)\n",
    "   - Fast training\n",
    "   - Requires labeled data\n",
    "\n",
    "3. **Zero-Shot with Embeddings**:\n",
    "   - No labeled data needed\n",
    "   - Good performance (~78-84%)\n",
    "   - Very flexible\n",
    "\n",
    "4. **Generative Models** (GPT, LLaMA via Groq):\n",
    "   - Natural language interface\n",
    "   - Easy to customize with prompts\n",
    "   - Slower but very flexible\n",
    "\n",
    "5. **T5 Models**:\n",
    "   - Unified text-to-text framework\n",
    "   - Good performance (~84%)\n",
    "   - Easy to adapt to new tasks\n",
    "\n",
    "**Key takeaways**:\n",
    "- Embeddings are powerful and versatile\n",
    "- Simple classifiers on embeddings can outperform complex models\n",
    "- Zero-shot is viable for many tasks\n",
    "- Choose approach based on: data availability, performance needs, flexibility requirements"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
